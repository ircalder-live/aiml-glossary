
[
  {
    "id": 1,
    "term": "Supervised Learning",
    "definition": "A type of machine learning where the model is trained on labeled data to learn a mapping from inputs to outputs.",
    "tags": ["machine learning", "labeled data", "prediction"],
    "related_terms": ["Classification", "Regression"],
    "examples": ["Predicting house prices using historical sales data."],
    "source": "Scikit-learn documentation",
    "last_updated": "2025-10-23"
  },
  {
    "id": 2,
    "term": "Unsupervised Learning",
    "definition": "A machine learning approach where the model identifies patterns in data without labeled outcomes.",
    "tags": ["clustering", "dimensionality reduction", "pattern discovery"],
    "related_terms": ["Clustering", "Principal Component Analysis"],
    "examples": ["Segmenting customers based on purchasing behavior."],
    "source": "Hastie, Tibshirani & Friedman (2009). The Elements of Statistical Learning.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 3,
    "term": "Classification",
    "definition": "A supervised learning task where the goal is to assign discrete labels to input data.",
    "tags": ["supervised learning", "discrete output", "prediction"],
    "related_terms": ["Logistic Regression", "Decision Tree"],
    "examples": ["Email spam detection, disease diagnosis."],
    "source": "Bishop, C. M. (2006). Pattern Recognition and Machine Learning.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 4,
    "term": "Regression",
    "definition": "A supervised learning task where the goal is to predict continuous numerical values.",
    "tags": ["supervised learning", "continuous output", "prediction"],
    "related_terms": ["Linear Regression", "Polynomial Regression"],
    "examples": ["Predicting temperature based on historical weather data."],
    "source": "James et al. (2013). An Introduction to Statistical Learning.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 5,
    "term": "Clustering",
    "definition": "An unsupervised learning technique that groups data points into clusters based on similarity.",
    "tags": ["unsupervised learning", "grouping", "similarity"],
    "related_terms": ["K-Means", "Hierarchical Clustering"],
    "examples": ["Grouping news articles by topic."],
    "source": "Jain, A. K. (2010). Data clustering: 50 years beyond K-means.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 6,
    "term": "Data Cleaning",
    "definition": "The process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset to improve data quality.",
    "tags": ["data preprocessing", "quality assurance", "missing values"],
    "related_terms": ["Outlier Detection", "Missing Value Imputation"],
    "examples": ["Removing duplicate rows, handling missing values, correcting typos in categorical data."],
    "source": "Kelleher, J.D., Namee, B., & D'Arcy, A. (2015). Fundamentals of Machine Learning for Predictive Data Analytics.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 7,
    "term": "Feature Scaling",
    "definition": "A technique used to normalize the range of independent variables or features of data to ensure uniformity and improve model performance.",
    "tags": ["normalization", "standardization", "preprocessing"],
    "related_terms": ["Min-Max Scaling", "StandardScaler"],
    "examples": ["Scaling features to a range of [0,1] using Min-Max normalization."],
    "source": "Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 8,
    "term": "Dimensionality Reduction",
    "definition": "The process of reducing the number of input variables in a dataset to simplify models and reduce computational cost.",
    "tags": ["feature engineering", "PCA", "data compression"],
    "related_terms": ["Principal Component Analysis", "t-SNE"],
    "examples": ["Using PCA to reduce a dataset from 100 features to 10 principal components."],
    "source": "Jolliffe, I.T., & Cadima, J. (2016). Principal component analysis: a review and recent developments.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 9,
    "term": "Encoding Categorical Variables",
    "definition": "The process of converting categorical data into numerical format so that it can be used in machine learning models.",
    "tags": ["categorical data", "preprocessing", "encoding"],
    "related_terms": ["One-Hot Encoding", "Label Encoding"],
    "examples": ["Transforming 'Color' feature with values ['Red', 'Blue', 'Green'] into one-hot encoded vectors."],
    "source": "Pedregosa et al. (2011). Scikit-learn: Machine Learning in Python.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 10,
    "term": "Feature Extraction",
    "definition": "The process of transforming raw data into informative features that can be used for model training.",
    "tags": ["feature engineering", "preprocessing", "transformation"],
    "related_terms": ["Feature Selection", "Dimensionality Reduction"],
    "examples": ["Extracting edge features from images using Sobel filters."],
    "source": "Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 11,
    "term": "Cross-Validation",
    "definition": "A model evaluation technique that partitions the dataset into multiple subsets, training the model on some and validating it on others to assess generalization performance.",
    "tags": ["model evaluation", "validation", "resampling"],
    "related_terms": ["Train-Test Split", "K-Fold Cross-Validation"],
    "examples": ["Using 5-fold cross-validation to evaluate a classifier's accuracy across different data splits."],
    "source": "Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 12,
    "term": "Confusion Matrix",
    "definition": "A table used to describe the performance of a classification model by showing the counts of true positives, false positives, true negatives, and false negatives.",
    "tags": ["classification", "evaluation", "metrics"],
    "related_terms": ["Precision", "Recall", "F1 Score"],
    "examples": ["A binary classifier predicting spam emails yields a confusion matrix with 80 true positives, 10 false positives, 5 false negatives, and 105 true negatives."],
    "source": "Powers, D.M.W. (2011). Evaluation: From precision, recall and F-measure to ROC, informedness, markedness and correlation.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 13,
    "term": "Precision",
    "definition": "The ratio of true positive predictions to the total number of positive predictions made by the model, indicating the accuracy of positive predictions.",
    "tags": ["classification", "metrics", "evaluation"],
    "related_terms": ["Recall","F1 Score", "Confusion Matrix"],
    "examples": ["If a model predicts 100 positive cases and 80 of them are correct, the precision is 0.80."],
    "source": "Sokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 14,
    "term": "Recall",
    "definition": "The ratio of true positive predictions to the total number of actual positive instances, measuring the model's ability to identify positive cases.",
    "tags": ["classification", "metrics", "evaluation"],
    "related_terms": ["Precision", "F1 Score", "Confusion Matrix"],
    "examples": ["If there are 100 actual positive cases and the model correctly identifies 80, the recall is 0.80."],
    "source": "Sokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 15,
    "term": "F1 Score",
    "definition": "The harmonic mean of precision and recall, providing a single metric that balances both concerns in classification tasks.",
    "tags": ["classification", "metrics", "evaluation"],
    "related_terms": ["Precision", "Recall", "Confusion Matrix"],
    "examples": ["If a model has precision 0.80 and recall 0.70, the F1 score is approximately 0.746."],
    "source": "Powers, D.M.W. (2011). Evaluation: From precision, recall and F-measure to ROC, informedness, markedness and correlation.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 16,
    "term": "Hyperparameter Tuning",
    "definition": "The process of optimizing the configuration parameters that are not learned during training but affect model performance.",
    "tags": ["optimization", "model tuning", "hyperparameters"],
    "related_terms": ["Grid Search", "Random Search", "Bayesian Optimization"],
    "examples": ["Adjusting the learning rate and number of layers in a neural network to improve accuracy."],
    "source": "Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 17,
    "term": "Grid Search",
    "definition": "An exhaustive search method that evaluates all possible combinations of hyperparameter values in a specified grid.",
    "tags": ["hyperparameter tuning", "grid search", "model selection"],
    "related_terms": ["Random Search", "Cross-Validation"],
    "examples": ["Testing combinations of kernel types and regularization parameters in an SVM."],
    "source": "Pedregosa et al. (2011). Scikit-learn: Machine Learning in Python.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 18,
    "term": "Random Search",
    "definition": "A hyperparameter optimization technique that samples random combinations of parameters from a predefined distribution.",
    "tags": ["hyperparameter tuning", "random sampling", "optimization"],
    "related_terms": ["Grid Search", "Bayesian Optimization"],
    "examples": ["Randomly selecting values for dropout rate and batch size to find optimal settings."],
    "source": "Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 19,
    "term": "Bayesian Optimization",
    "definition": "An optimization technique that builds a probabilistic model of the objective function and uses it to select the most promising hyperparameters.",
    "tags": [ "hyperparameter tuning", "bayesian methods", "optimization"],
    "related_terms": ["Gaussian Process", "Acquisition Function"],
    "examples": ["Using Bayesian optimization to tune the learning rate and momentum in a neural network."],
    "source": "Snoek, J., Larochelle, H., & Adams, R.P. (2012). Practical Bayesian optimization of machine learning algorithms.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 20,
    "term": "Early Stopping",
    "definition": "A regularization technique that stops training when the model's performance on a validation set starts to degrade.",
    "tags": [ "regularization", "model tuning", "overfitting prevention"],
    "related_terms": ["Validation Set", "Overfitting"],
    "examples": ["Halting training after 10 epochs when validation loss begins to increase."],
    "source": "Prechelt, L. (1998). Early stopping—but when? In Neural Networks: Tricks of the Trade.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 21,
    "term": "Artificial Neural Network",
    "definition": "A computational model inspired by the structure and function of biological neural networks, consisting of interconnected nodes (neurons) organized in layers.",
    "tags": ["deep learning", "neural networks", "machine learning"],
    "related_terms": ["Perceptron", "Multilayer Perceptron"],
    "examples": ["Using an ANN to classify handwritten digits in the MNIST dataset."],
    "source": "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 22,
    "term": "Activation Function",
    "definition": "A mathematical function applied to a neuron's output to introduce non-linearity into the model, enabling learning of complex patterns.",
    "tags": ["neural networks", "non-linearity", "deep learning"],
    "related_terms": ["ReLU", "Sigmoid", "Tanh"],
    "examples": ["Using ReLU activation in hidden layers of a CNN."],
    "source": "Nwankpa, C., et al. (2018). Activation Functions: Comparison and Usage.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 23,
    "term": "Backpropagation",
    "definition": "An algorithm used to train neural networks by computing gradients of the loss function and updating weights via gradient descent.",
    "tags": ["training", "optimization", "neural networks"],
    "related_terms": ["Gradient Descent", "Loss Function"],
    "examples": ["Training a neural network using backpropagation with stochastic gradient descent."],
    "source": "Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986). Learning representations by back-propagating errors.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 24,
    "term": "Convolutional Neural Network",
    "definition": "A type of deep neural network designed to process structured grid data such as images, using convolutional layers to extract spatial features.",
    "tags": ["deep learning", "computer vision", "CNN"],
    "related_terms": ["Pooling", "Feature Maps"],
    "examples": ["Using a CNN to classify images in the CIFAR-10 dataset."],
    "source": "LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 25,
    "term": "Recurrent Neural Network",
    "definition": "A class of neural networks where connections between nodes form directed cycles, allowing modeling of sequential data.",
    "tags": ["sequence modeling", "RNN", "deep learning"],
    "related_terms": ["LSTM", "GRU"],
    "examples": ["Using an RNN to predict the next word in a sentence."],
    "source": "Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 26,
    "term": "Model Deployment",
    "definition": "The process of integrating a trained machine learning model into a production environment where it can make predictions on real-world data.",
    "tags": ["deployment", "production", "ML lifecycle"],
    "related_terms": ["Model Serving", "API"],
    "examples": ["Deploying a fraud detection model as a REST API."],
    "source": "Sculley, D., et al. (2015). Hidden Technical Debt in Machine Learning Systems.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 27,
    "term": "Model Serving",
    "definition": "The infrastructure and tools used to host and deliver machine learning models for inference in production environments.",
    "tags": ["deployment", "inference", "scalability"],
    "related_terms": ["Model Deployment", "REST API"],
    "examples": ["Using TensorFlow Serving to host a trained model."],
    "source": "TensorFlow Serving documentation.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 28,
    "term": "Monitoring",
    "definition": "The practice of tracking model performance and system metrics in production to detect issues such as data drift or latency.",
    "tags": ["observability", "performance", "ML operations"],
    "related_terms": ["Concept Drift", "Logging"],
    "examples": ["Monitoring prediction accuracy and response time of a deployed model."],
    "source": "Baylor, D., et al. (2017). TFX: A TensorFlow-Based Production-Scale Machine Learning Platform.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 29,
    "term": "A/B Testing",
    "definition": "An experimental technique to compare two versions of a model or system by exposing users to each and measuring performance differences.",
    "tags": ["experimentation", "evaluation", "deployment"],
    "related_terms": ["Model Comparison", "Online Testing"],
    "examples": ["Testing two recommendation models to determine which yields higher click-through rates."],
    "source": "Kohavi, R., et al. (2009). Controlled experiments on the web: survey and practical guide.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 30,
    "term": "Rollback",
    "definition": "The process of reverting to a previous model version or system state in response to performance degradation or failure.",
    "tags": ["deployment", "reliability", "version control"],
    "related_terms": ["Model Versioning", "Monitoring"],
    "examples": ["Rolling back to a previous model after detecting a drop in accuracy."],
    "source": "Sculley, D., et al. (2015). Hidden Technical Debt in Machine Learning Systems.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 31,
    "term": "Concept Drift",
    "definition": "A phenomenon where the statistical properties of the target variable change over time, affecting model performance.",
    "tags": ["stream learning", "non-stationarity", "adaptation"],
    "related_terms": ["Data Drift", "Model Monitoring"],
    "examples": ["Customer behavior changes over time, impacting churn prediction models."],
    "source": "Gama, J., et al. (2014). A survey on concept drift adaptation.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 32,
    "term": "Temporal Dependency",
    "definition": "The relationship between data points across time, where past values influence future outcomes.",
    "tags": ["sequence modeling", "time series", "stream learning"],
    "related_terms": ["Autocorrelation", "Lag Features"],
    "examples": ["Stock prices influenced by previous days' values."],
    "source": "Hamilton, J.D. (1994). Time Series Analysis.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 33,
    "term": "Replay Methods",
    "definition": "Techniques in continual learning that store and replay past data to mitigate forgetting and improve stability.",
    "tags": ["continual learning", "memory","rehearsal"],
    "related_terms": ["Experience Replay", "Episodic Memory"],
    "examples": ["Using a buffer of past samples to retrain a model incrementally."],
    "source": "Rolnick, D., et al. (2019). Experience Replay for Continual Learning.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 34,
    "term": "Regularization Methods",
    "definition": "Approaches that constrain model updates to preserve previously learned knowledge during continual learning.",
    "tags": ["continual learning", "stability", "regularization"],
    "related_terms": ["Elastic Weight Consolidation", "L2 Regularization"],
    "examples": ["Using EWC to prevent catastrophic forgetting in neural networks."],
    "source": "Kirkpatrick, J., et al. (2017). Overcoming catastrophic forgetting in neural networks.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 35,
    "term": "Isolation Methods",
    "definition": "Strategies that allocate separate model components or parameters for different tasks to avoid interference.",
    "tags": ["continual learning", "modularity", "task separation"],
    "related_terms": ["Progressive Networks", "Dynamic Architectures"],
    "examples": ["Using task-specific subnetworks to isolate learning."],
    "source": "Rusu, A.A., et al. (2016). Progressive neural networks.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 36,
    "term": "Symbolic Regression",
    "definition": "A type of regression analysis that searches for mathematical expressions that best fit a dataset using symbolic representations.",
    "tags": ["symbolic AI", "regression", "genetic programming"],
    "related_terms": ["Genetic Programming", "Expression Trees"],
    "examples": ["Using symbolic regression to discover physical laws from experimental data."],
    "source": "Koza, J.R. (1992). Genetic Programming: On the Programming of Computers by Means of Natural Selection.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 37,
    "term": "Genetic Programming",
    "definition": "An evolutionary algorithm-based methodology that evolves computer programs to solve problems by mimicking natural selection.",
    "tags": ["evolutionary algorithms", "symbolic AI", "optimization"],
    "related_terms": ["Genetic Algorithms", "Symbolic Regression"],
    "examples": ["Evolving a sorting algorithm using genetic programming."],
    "source": "Koza, J.R. (1992). Genetic Programming: On the Programming of Computers by Means of Natural Selection.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 38,
    "term": "Randomized Algorithms",
    "definition": "Algorithms that use random numbers to influence their behavior, often yielding faster or simpler solutions.",
    "tags": ["probabilistic algorithms", "complexity", "optimization"],
    "related_terms": ["Monte Carlo Methods", "Las Vegas Algorithms"],
    "examples": ["Using randomized quicksort to improve average-case performance."],
    "source": "Motwani, R., & Raghavan, P. (1995). Randomized Algorithms.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 39,
    "term": "Computational Complexity",
    "definition": "A field of study that analyzes the resources required for algorithms to solve problems, such as time and space.",
    "tags": ["theoretical computer science", "efficiency", "algorithms"],
    "related_terms": ["Big O Notation", "P vs NP"],
    "examples": ["Analyzing the time complexity of sorting algorithms."],
    "source": "Sipser, M. (2012). Introduction to the Theory of Computation.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 40,
    "term": "Symbolic AI",
    "definition": "An approach to artificial intelligence that uses high-level, human-readable symbols and rules to represent knowledge and reasoning.",
    "tags": ["knowledge_representation","logic","expert systems"],
    "related_terms": ["Rule-Based Systems","Ontologies","Knowledge Graphs"],
    "examples": ["Using a rule-based system to diagnose medical conditions."],
    "source": "Russell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 42,
    "term": "Skipgram",
    "definition": "Skipgram is a Word2Vec architecture that learns word embeddings by predicting surrounding context words given a target word. It is effective for learning representations of rare words.",
    "tags": ["natural_language_processing", "word_embeddings", "neural networks", "representation_learning"],
    "related_terms": ["Word2Vec", "CBOW", "Word Embedding", "Distribution Hypothesis"],
    "examples": ["Predicting 'dog' and 'barks' given the target word 'loudly' in the sentence 'The dog barks loudly.'"],
    "source": "Mikolov et al. (2013), arXiv:1301.3781; Wikipedia: https://en.wikipedia.org/wiki/Word2vec",
    "last_updated": "2025-10-23"
  },
  {
    "id": 43,
    "term": "Word2Vec",
    "definition": "Word2Vec is a neural embedding model that learns vector representations of words based on their context in large corpora. It uses architectures like CBOW and Skip-gram to predict words or contexts.",
    "tags": ["natural_language_processing", "word embedding", "neural_networks", "representation_learning"],
    "related_terms": ["Skipgram", "CBOW", "Word Embedding", "Distribution Hypothesis"],
    "examples": [],
    "source": "Mikolov et al. (2013), arXiv:1301.3781; Wikipedia: https://en.wikipedia.org/wiki/Word2vec",
    "last_updated": "2025-10-23"
  },
  {
    "id": 44,
    "term": "Distribution Hypothesis",
    "definition": "The distributional hypothesis states that words appearing in similar contexts tend to have similar meanings. It underpins many NLP techniques including word embeddings and semantic similarity models.",
    "tags": ["natural_language_processing", "semantics", "distributional_semantics", "representation_learning"],
    "related_terms": ["Word Embedding", "Word2Vec", "Semantic Similarity", "Contextual Embedding"],
    "examples": [],
    "source": "Firth (1957); Wikipedia: https://en.wikipedia.org/wiki/Distributional_semantics",
    "last_updated": "2025-10-23"
  },
  {
    "id": 45,
    "term": "Text Representation",
    "definition": "Text representation involves converting raw text into numerical formats suitable for machine learning. Techniques include bag-of-words, TF-IDF, and embeddings like Word2Vec and BERT.",
    "tags": ["natural_language_processing", "text_encoding", "feature_extraction", "representation_learning"],
    "related_terms": ["Word Embedding", "Word Representation", "TF-IDF", "Bag-of-Words"],
    "examples": [],
    "source": "Jurafsky & Martin (2023), Speech and Language Processing; Wikipedia: https://en.wikipedia.org/wiki/Text_representation",
    "last_updated": "2025-10-23"
  },
  {
    "id": 46,
    "term": "Word Representation",
    "definition": "Word representation refers to the way words are encoded for use in computational models. Traditional methods include one-hot encoding and TF-IDF, while modern approaches use dense embeddings that capture semantic relationships.",
    "tags": ["natural_language_processing", "representation_learning", "word_embedding", "semantic_similarity"],
    "related_terms": ["Word Embedding", "Word2Vec", "Distributional Semantics", "Text Representation"],
    "examples": [],
    "source": "Jurafsky & Martin (2023), Speech and Language Processing; Wikipedia: https://en.wikipedia.org/wiki/Word_representation",
    "last_updated": "2025-10-23"
  },
  {
    "id": 47,
    "term": "Simulated Annealing",
    "definition": "A probabilistic optimization technique inspired by metallurgy's annealing process. It explores solution spaces by accepting worse solutions with a probability that decreases over time to avoid local optima.",
    "tags": ["optimization","metaheuristic", "probabilistic_algorithm", "search"],
    "related_terms": ["Local Search", "Metropolis Algorithm", "Cooling Schedule", "Hill Climbing"],
    "examples": [],
    "source": "GeeksforGeeks, Cornell Open Textbook",
    "last_updated": "2025-10-23"
  },
  {
    "id": 48,
    "term": "Monte Carlo Algorithm",
    "definition": "A randomized method that runs in fixed time and may yield incorrect results with some probability; error can be reduced via repetition.",
    "tags": ["randomized_algorithm", "probabilistic", "algorithm_design"],
    "related_terms": ["Las Vegas Algorithm", "Randomized Algorithms"],
    "examples": [],
    "source": "Duke University COMPSCI 330, CMU Harchol-Balter",
    "last_updated": "2025-10-23"
  },
  {
    "id": 49,
    "term": "Las Vegas Algorithm",
    "definition": "A randomized algorithm that always returns the correct result (or explicitly indicates failure), but whose runtime is probabilistic. It must have finite expected runtime.",
    "tags": ["randomized_algorithm", "probabilistic", "algorithm_design"],
    "related_terms": ["Monte Carlo Algorithm", "Randomized Algorithms"],
    "examples": [],
    "source": "Wikipedia 'Las Vegas algorithm', Canny (1999 lecture)",
    "last_updated": "2025-10-23"
  },
  {
    "id": 50,
    "term": "Pseudocode",
    "definition": "A human-readable, high-level description of an algorithm combining natural language and programming-like structures. It is more precise than plain English but less formal than executable code.",
    "tags": ["algorithm", "documentation", "algorithm_design", "programming"],
    "related_terms": ["Flowchart", "Algorithm", "Code", "Pseudocode Syntax"],
    "examples": [],
    "source": "de Weerdt (2019), Wikipedia 'Pseudocode'",
    "last_updated": "2025-10-23"
  },
  {
    "id": 51,
    "term": "Continual Learning",
    "definition": "Continual Learning (also known as lifelong learning) refers to the ability of a machine learning system to learn from a continuous stream of data over time, adapting to new tasks while retaining knowledge from previous ones. The key challenge in continual learning is avoiding catastrophic forgetting — the tendency of neural networks to forget previously learned information when trained on new data.",
    "tags": ["machine_learning", "lifelong_learning", "online_learning", "catastrophic_forgetting", "adaptive_models"],
    "related_terms": ["Catastrophic Forgetting", "Elastic Weight Consolidation (EWC)", "Replay Buffer", "Online Learning", "Transfer Learning", "Meta-Learning"],
    "examples": [],
    "source": "Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., & Wermter, S. (2019). *Continual lifelong learning with neural networks: A review*. Neural Networks.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 52,
    "term": "Random Forest",
    "definition": "Random Forest is an ensemble learning algorithm that constructs a multitude of decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It combines the principles of bagging (bootstrap aggregating) and random feature selection to reduce variance and improve generalization. Random Forests are robust to overfitting and can handle high-dimensional data effectively.",
    "tags": ["machine_learning,"],
    "related_terms": ["Decision Tree", "Bagging", "Ensemble Learning", "Feature Importance", "Out-of-Bag Error"],
    "examples": [],
    "source": "Breiman, L. (2001). *Random Forests*. Machine Learning.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 53,
    "term": "Adaptive Random Forest (ARF)",
    "definition": "Adaptive Random Forest (ARF) is an ensemble learning algorithm designed for evolving data streams. It extends the traditional Random Forest by incorporating mechanisms to detect and adapt to concept drift — changes in the underlying data distribution over time. ARF uses a collection of decision trees trained incrementally on streaming data, with each tree equipped with a drift detector and a background learner to replace outdated models when drift is detected.",
    "tags": ["machine_learning", "stream_learning", "ensemble_learning", "concept_drift", "adaptive_models"],
    "related_terms": ["Random Forest", "Concept Drift", "Online Learning", "Drift Detection Method (DDM)", "Hoeffding Tree", "Streaming Data"],
    "examples": [],
    "source": "Gomes, H. M., Bifet, A., Read, J., Barddal, J. P., Enembreck, F., Pfharinger, B., Holmes, G., & Abdessalem, T. (2017). *Adaptive Random Forests for Evolving Data Stream Classification*. Machine Learning Journal.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 54,
    "term": "Ensembles",
    "definition": "In machine learning, an ensemble refers to a collection of models whose predictions are combined to produce a final output. The goal is to leverage the strengths of multiple models to improve accuracy, robustness, and generalization. Ensembles can be homogeneous (same model type) or heterogeneous (different model types), and they often outperform individual models, especially in competitions and real-world applications.",
    "tags": ["machine_learning", "ensemble_learning", "model_combination", "classification", "regression"],
    "related_terms": ["Bagging"],
    "examples": [],
    "source": "Zhou, Z.-H. (2012). *Ensemble Methods: Foundations and Algorithms*.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 55,
    "term": "Naive Bayes",
    "definition": "Naive Bayes is a family of probabilistic classifiers based on Bayes’ Theorem, assuming strong (naive) independence between features. Despite this simplifying assumption, Naive Bayes classifiers often perform competitively in real-world tasks, especially in text classification and spam detection, due to their efficiency and robustness to irrelevant features.",
    "tags": [
      "machine_learning,"
    ],
    "related_terms": ["Bayes’ Theorem", "Conditional Probability", "Multinomial Naive Bayes", "Gaussian Naive Bayes", "Bernoulli Naive Bayes", "Logistic Regression"],
    "examples": [],
    "source": "Mitchell, T. M. (1997). *Machine Learning*. McGraw-Hill.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 56,
    "term": "Decision Tree",
    "definition": "A decision tree is a supervised learning model used for classification and regression tasks. It represents decisions and their possible consequences as a tree-like structure, where internal nodes represent feature-based tests, branches represent outcomes of those tests, and leaf nodes represent predicted outputs. Decision trees are interpretable, non-parametric models that recursively partition the input space to minimize impurity or error.",
    "tags": ["machine_learning", "classification", "regression", "tree_model", "interpretable_model"],
    "related_terms": ["Random Forest", "CART (Classification and Regression Trees)", "Gini Impurity", "Entropy", "Pruning", "Overfitting"],
    "examples": [],
    "source": "Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). *Classification and Regression Trees*.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 57,
    "term": "k-Nearest Neighbors (k-NN)",
    "definition": "k-Nearest Neighbors (kNN) is a non-parametric, instance-based learning algorithm used for classification and regression. It predicts the label of a new data point by identifying the kkk closest training examples in the feature space and using their labels to make a decision — typically via majority vote (classification) or averaging (regression). It assumes that similar instances exist in close proximity in the feature space.",
    "tags": ["machine_learning", "classification", "regression", "instance_based_learning", "non_parametric"],
    "related_terms": ["Distance Metrics", "Lazy Learning", "Decision Boundary", "Curse of Dimensionality", "Weighted kNN"],
    "examples": [],
    "source": "Cover, T. M., & Hart, P. E. (1967). *Nearest neighbor pattern classification. IEEE Transactions on Information Theory*.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 58,
    "term": "Adaptive Machine Learning (Adaptive ML)",
    "definition": "Adaptive Machine Learning refers to systems that dynamically learn and update their models in response to new or changing data without requiring explicit retraining on static datasets. These adaptive systems continuously ingest and process information—whether in real-time or batches—to adjust to evolving environments, detect and react to concept drift, and maintain accuracy over time.",
    "tags": ["machine_learning", "online_learning", "concept_drift", "real_time_learning", "stream_learning", "adaptive_systems"],
    "related_terms": ["Online Learning", "Incremental Learning", "Concept Drift", "Continual Learning", "Streaming Data", "Reinforcement Learning"],
    "examples": [],
    "source": "Neha Singh & Khushi Chugh (2025), *“What is Adaptive Machine Learning and How Does It Work?”* (Pickl.ai) [pickl.ai]",
    "last_updated": "2025-10-23"
  },
  {
    "id": 59,
    "term": "Explainable AI (XAI)",
    "definition": "Explainable AI (XAI) comprises methods and processes designed to make the behavior, decisions, and internal reasoning of AI systems — especially complex \"black-box\" models — transparent, understandable, and auditable to human users. XAI seeks to build trust, enable accountability, and support oversight by revealing how inputs influence outputs based on interpretable artifacts like feature importances, saliency maps, or example-based explanations.",
    "tags": ["machine_learning", "interpretability", "transparency", "model_explainability", "trustworthy_AI", "auditing"],
    "related_terms": ["Interpretability", "Transparency", "Post-hoc explanations", "Ante-hoc (intrinsic) interpretable models", "Saliency mapping", "Counterfactual explanations"],
    "examples": [],
    "source": "Wikipedia: *“Explainable artificial intelligence”* [en.wikipedia.org]",
    "last_updated": "2025-10-23"
  },
  {
    "id": 60,
    "term": "Bootstrap Aggregating (Bagging)",
    "definition": "Bootstrap Aggregating, or Bagging, is an ensemble learning technique that improves the stability and accuracy of machine learning algorithms by training multiple models on different bootstrapped subsets of the training data and aggregating their predictions. It reduces variance and helps prevent overfitting, especially in high-variance models like decision trees.",
    "tags": ["ensemble_learning", "bagging", "bootstrap", "variance_reduction", "machine_learning"],
    "related_terms": ["Random Forest", "Bootstrap Sampling", "Ensemble Learning", "Variance", "Decision Tree"],
    "examples": [],
    "source": "Breiman, L. (1996). *Bagging Predictors*. Machine Learning.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 61,
    "term": "Ensemble Learning",
    "definition": "Ensemble learning is a machine learning paradigm where multiple models (often called \"base learners\" or \"weak learners\") are trained and combined to solve the same problem. The goal is to improve predictive performance, robustness, and generalization by aggregating the outputs of diverse models. Ensembles can reduce variance (e.g., bagging), bias (e.g., boosting), or improve predictions through voting or averaging.",
    "tags": [
      "machine_learning",
      "ensemble_methods",
      "model_combination",
      "bagging",
      "boosting"
    ],
    "related_terms": [
      "Bagging",
      "Boosting",
      "Stacking",
      "Random Forest",
      "Voting Classifier",
      "Bootstrap Aggregating"
    ],
    "examples": [],
    "source": "Zhou, Z.-H. (2012). *Ensemble Methods: Foundations and Algorithms*.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 62,
    "term": "Tournament Selection",
    "definition": "Tournament selection is a method used in evolutionary algorithms to select individuals for reproduction. A subset of individuals is randomly chosen from the population, and the one with the highest fitness among them is selected. This process is repeated to fill the mating pool. Tournament size controls selection pressure: larger tournaments increase the chance of selecting fitter individuals.",
    "tags": [
      "evolutionary_algorithm",
      "selection_method",
      "genetic_algorithm",
      "genetic_programming",
      "fitness_based_selection"
    ],
    "related_terms": [
      "Fitness Function",
      "Lexicase Selection",
      "Roulette Wheel Selection",
      "Rank Selection",
      "Elitism"
    ],
    "examples": [],
    "source": "Goldberg, D. E., & Deb, K. (1991). *A comparative analysis of selection schemes used in genetic algorithms*.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 63,
    "term": "Parsimony Pressure",
    "definition": "Parsimony pressure is a technique used in genetic programming and evolutionary algorithms to control bloat by penalizing overly complex solutions. It modifies the fitness function or selection criteria to favor simpler individuals, encouraging the evolution of more compact and generalizable models without sacrificing performance.",
    "tags": [
      "genetic_programming",
      "evolutionary_algorithm",
      "bloat_control",
      "model_complexity",
      "regularization"
    ],
    "related_terms": [
      "Bloat",
      "Fitness Function",
      "Regularization",
      "Minimum Description Length (MDL)",
      "Introns"
    ],
    "examples": [],
    "source": "Poli, R., Langdon, W. B., & McPhee, N. F. (2008). *A Field Guide to Genetic Programming*",
    "last_updated": "2025-10-23"
  },
  {
    "id": 64,
    "term": "Bloat",
    "definition": "Bloat refers to the uncontrolled growth of solution representations (e.g., expression trees in genetic programming) without corresponding improvements in fitness. It often results in overly complex, inefficient, and less generalizable models. Bloat is a common issue in genetic programming due to the tendency of evolutionary operators like crossover and mutation to increase program size over generations.",
    "tags": [
      "genetic_programming",
      "evolutionary_algorithm",
      "model_complexity",
      "overfitting",
      "optimization"
    ],
    "related_terms": [
      "Parsimony Pressure",
      "Overfitting",
      "Genetic Drift",
      "Introns",
      "Code Growth"
    ],
    "examples": [],
    "source": "Poli, R., Langdon, W. B., & McPhee, N. F. (2008). *A Field Guide to Genetic Programming*.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 65,
    "term": "Underfitting",
    "definition": "Underfitting occurs when a machine learning model is too simple to capture the underlying structure of the data. It results in poor performance on both the training and test sets. Underfitting is typically caused by high bias, insufficient model complexity, or inadequate training time.",
    "tags": [
      "machine_learning",
      "model_evaluation",
      "bias",
      "generalization",
      "training_error"
    ],
    "related_terms": [
      "Overfitting",
      "Generalization",
      "Bias-Variance Trade-off",
      "Model Complexity",
      "Regularization"
    ],
    "examples": [],
    "source": "Goodfellow, Bengio & Courville (2016), *Deep Learning*, MIT Press",
    "last_updated": "2025-10-23"
  },
  {
    "id": 66,
    "term": "Overfitting",
    "definition": "Overfitting occurs when a machine learning model learns not only the underlying patterns in the training data but also the noise and random fluctuations. As a result, the model performs well on training data but poorly on unseen data. Overfitting is a key obstacle to generalization and is more likely when models are too complex relative to the amount of training data.",
    "tags": [
      "machine_learning",
      "model_evaluation",
      "generalization",
      "bias_variance",
      "training_error"
    ],
    "related_terms": [
      "Underfitting",
      "Generalization",
      "Bias-Variance Trade-off",
      "Regularization",
      "Cross-Validation"
    ],
    "examples": [],
    "source": "Goodfellow, Bengio & Courville (2016), *Deep Learning*, MIT Press",
    "last_updated": "2025-10-23"
  },
  {
    "id": 67,
    "term": "Bias-Variance Trade-off",
    "definition": "The bias-variance trade-off is a fundamental concept in statistical learning theory that describes the tension between two sources of error in predictive models:",
    "tags": [
      "machine_learning model_evaluation",
      "generalization",
      "overfitting",
      "underfitting"
    ],
    "related_terms": [
      "Overfitting",
      "Underfitting",
      "Generalization",
      "Regularization",
      "Cross-Validation"
    ],
    "examples": [],
    "source": "Geman, S., Bienenstock, E., & Doursat, R. (1992). *Neural networks and the bias/variance dilemma*. Neural Computation.",
    "last_updated": "2025-10-23"
  },
  {
    "id": 68,
    "term": "Generalization",
    "definition": "In machine learning, generalization refers to a model’s ability to perform well on unseen data drawn from the same distribution as the training set. A well-generalized model captures the underlying patterns in the data without overfitting to noise or specific examples. Generalization is the ultimate goal of supervised learning and is typically evaluated using validation or test sets.",
    "tags": [
      "machine_learning",
      "model_evaluation",
      "overfitting",
      "bias_variance",
      "generalization_error"
    ],
    "related_terms": [
      "Overfitting",
      "Underfitting",
      "Bias-Variance Trade-off",
      "Cross-Validation",
      "Test Error"
    ],
    "examples": [],
    "source": "Goodfellow, Bengio & Courville (2016), *Deep Learning*, MIT Press",
    "last_updated": "2025-10-23"
  },
  {
    "id": 69,
    "term": "Mutation",
    "definition": "Mutation is a genetic operator used in evolutionary algorithms to introduce random variation into individuals. It modifies parts of a solution (e.g., a gene, subtree, or node) to maintain genetic diversity and explore new areas of the search space. Mutation helps prevent premature convergence and supports exploration.",
    "tags": [
      "evolutionary_algorithm",
      "genetic_programming",
      "genetic_algorithm",
      "variation_operator",
      "diversity"
    ],
    "related_terms": [
      "Crossover",
      "Selection",
      "Genetic Drift",
      "Hill Climbing",
      "Exploration vs. Exploitation"
    ],
    "examples": [],
    "source": "",
    "last_updated": "2025-10-23"
  },
  {
    "id": 70,
    "term": "Crossover",
    "definition": "Crossover is a genetic operator used in evolutionary algorithms to combine the genetic information of two parent solutions to generate new offspring. It mimics biological reproduction by exchanging substructures (e.g., genes, subtrees, or substrings) between individuals, promoting exploration of the solution space.",
    "tags": [
      "evolutionary_algorithm",
      "genetic_programming",
      "genetic_algorithm",
      "recombination",
      "variation_operator"
    ],
    "related_terms": [
      "Mutation Selection",
      "Genetic Programming",
      "Subtree Crossover",
      "Uniform Crossover",
      "One-point Crossover"
    ],
    "examples": [],
    "source": "Koza, J. R. (1992). *Genetic Programming: On the Programming of Computers by Means of Natural Selection*",
    "last_updated": "2025-10-23"
  },
  {
    "id": 71,
    "term": "Lexicase Selection",
    "definition": "A parent (and sometimes survivor) selection method in evolutionary computation that processes training cases one at a time in random order. Instead of aggregating performance across all cases, it filters the population stepwise: individuals that perform best on the current case advance to be evaluated on the next case, and so on until only one remains or test cases are exhausted.",
    "tags": [
      "evolutionary_algorithm",
      "genetic_programming",
      "parent_selection",
      "non_aggregate_selection",
      "diversity_maintenance"
    ],
    "related_terms": [
      "Tournament Selection",
      "Fitness-Proportionate Selection",
      "ε-Lexicase Selection",
      "Down-Sampled Lexicase Selection",
      "Pareto Selection"
    ],
    "examples": [],
    "source": "",
    "last_updated": "2025-10-23"
  },
  {
    "id": 72,
    "term": "Fitness Function",
    "definition": "A fitness function is a specific kind of objective (or cost) function used in evolutionary algorithms (EAs). It assigns a numerical score to each candidate solution, indicating how well it satisfies the optimization goals. It guides selection, reproduction, and progression of the population toward better solutions.",
    "tags": [
      "evolutionary_algorithm",
      "optimization",
      "objective_function",
      "genetic_programming",
      "genetic_algorithm"
    ],
    "related_terms": [
      "Genetic Algorithm (GA)",
      "Genetic Programming (GP)",
      "Objective Function",
      "Cost Function",
      "Selection Methods (e.g.",
      "Tournament Selection",
      "Roulette Wheel)"
    ],
    "examples": [],
    "source": "Wikipedia: \"Fitness function\" [en.wikipedia.org]",
    "last_updated": "2025-10-23"
  },
  {
    "id": 73,
    "term": "Expression Tree",
    "definition": "A tree data structure representing mathematical expressions, where leaf nodes are operands (variables/constants) and internal nodes represent operators or functions. Widely used in symbolic regression and compiler design to manipulate, evaluate, or optimize expressions.",
    "tags": [
      "data_structure",
      "symbolic_regression",
      "compilers",
      "AST",
      "expression_parsing"
    ],
    "related_terms": [
      "Abstract Syntax Tree",
      "Symbolic Regression",
      "Crossover",
      "Mutation"
    ],
    "examples": [
      "Representation of $sin(x) + 0$.$5*log(x + 2)$: a tree with root $+$",
      "left child $sin(x)$",
      "right subtree $*$ with children $0.5$ and $log(x+2)$. [ml4science.com]"
    ],
    "source": "Wikipedia “Symbolic regression” (description of expression tree usage) [en.wikipedia.org]",
    "last_updated": "2025-10-23"
  },
  {
    "id": 74,
    "term": "Monte Carlo Tree Search (MCTS)",
    "definition": "A heuristic tree-based search algorithm employing Monte Carlo simulations and selective node expansion to tackle vast decision spaces. It alternates between selection, expansion, simulation, and backpropagation guided by the Upper Confidence Bounds for Trees (UCT) criterion.",
    "tags": [
      "search_algorithm",
      "heuristic_search",
      "game_ai",
      "probabilistic",
      "UCT",
      "tree_search"
    ],
    "related_terms": [
      "Heuristic Search",
      "UCT",
      "Game Theory",
      "Reinforcement Learning"
    ],
    "examples": [],
    "source": "Wikipedia “Monte Carlo tree search” [en.wikipedia.org]",
    "last_updated": "2025-10-23"
  },
  {
    "id": 75,
    "term": "Bayesian Symbolic Regression",
    "definition": "A probabilistic symbolic regression method that infers closed-form expressions by sampling from a posterior distribution over expression trees. It integrates prior beliefs, quantifies uncertainty, and promotes simplicity through a Bayesian framework, often implemented with MCMC or Sequential Monte Carlo.",
    "tags": [
      "symbolic_regression",
      "Bayesian_methods",
      "probabilistic_modeling",
      "uncertainty_quantification",
      "interpretability"
    ],
    "related_terms": [
      "Symbolic Regression",
      "Expression Trees",
      "Posterior Sampling",
      "MCMC",
      "SMC"
    ],
    "examples": [],
    "source": "Ying Jin et al., Bayesian Symbolic Regression (arXiv:1910.08892) [arxiv.org]",
    "last_updated": "2025-10-23"
  },
  {
    "id": 76,
    "term": "Service‑oriented Computing (SOC)",
    "definition": "A design paradigm where discrete, network‑accessible software services expose functionality via well‑defined interfaces, enabling loose coupling, interoperability, reusability, and service orchestration.",
    "tags": [
      "software_architecture",
      "services",
      "SOA",
      "integration"
    ],
    "related_terms": [
      "Microservices",
      "API",
      "Enterprise Service Bus (ESB)",
      "Service Registry",
      "Web Services"
    ],
    "examples": [],
    "source": "“Service‑oriented architecture”, IBM, GeeksforGeeks",
    "last_updated": "2025-10-23"
  },
  {
    "id": 77,
    "term": "Implicit Graphs",
    "definition": "Graph representations where vertices and edges are not explicitly stored but generated dynamically via computational rules or functions. Common in large state spaces like puzzles or game trees.",
    "tags": [
      "graph",
      "implicit_representation",
      "search",
      "state_space"
    ],
    "related_terms": [
      "Graph Search",
      "BFS",
      "DFS",
      "State‐Space Graphs"
    ],
    "examples": [],
    "source": "Wikipedia “Implicit graph”, expert guide",
    "last_updated": "2025-10-23"
  },
  {
    "id": 78,
    "term": "Depth First Search (DFS)",
    "definition": "A graph/traversal search that explores as far down each branch as possible before backtracking, typically implemented recursively or with a stack. It visits each reachable node once using backtracking order.",
    "tags": [
      "graph_search",
      "depth_first",
      "traversal",
      "stack",
      "recursive"
    ],
    "related_terms": [
      "BFS",
      "Graph Search",
      "Connected Components",
      "Backtracking"
    ],
    "examples": [
      "On a branching graph from node 0: visits 0→1→2→3→4, returning and continuing depth‐wise at each node."
    ],
    "source": "Wikipedia “Depth-first search”, GeeksforGeeks [en.wikipedia.org], [geeksforgeeks.org] [geeksforgeeks.org], [en.wikipedia.org]",
    "last_updated": "2025-10-23"
  },
  {
    "id": 79,
    "term": "Breadth First Search (BFS)",
    "definition": "A graph‐traversal algorithm that explores vertices level by level, using a FIFO queue. It marks visited nodes to prevent repetition and can be used to find shortest paths in unweighted graphs.",
    "tags": [
      "graph_search",
      "breadth_first",
      "traversal",
      "shortest_path"
    ],
    "related_terms": [
      "Depth First Search",
      "Graph Search",
      "Queue",
      "Connected Components"
    ],
    "examples": [],
    "source": "Wikipedia “Breadth-first search”, GeeksforGeeks  [geeksforgeeks.org], [en.wikipedia.org] [en.wikipedia.org], [geeksforgeeks.org]",
    "last_updated": "2025-10-23"
  },
  {
    "id": 80,
    "term": "Graph Connectivity",
    "definition": "In graph theory, connectivity measures whether vertices are reachable from each other. In undirected graphs, connectedness means there's a path between every vertex pair. Vertex‐connectivity (κ) is the minimum number of vertices whose removal disconnects the graph; edge‐connectivity (λ) is similarly defined for edges.",
    "tags": [
      "graph_theory",
      "connectivity",
      "resilience",
      "network"
    ],
    "related_terms": [
      "Connected Component",
      "Vertex Cut",
      "Edge Cut",
      "k‑connected Graph"
    ],
    "examples": [
      "A tree graph is 1‑edge‑connected: removing any edge disconnects it. A complete graph Kₙ has κ = λ = n − 1."
    ],
    "source": "Wikipedia “Connectivity (graph theory)”, Algocademy [en.wikipedia.org], [algocademy.com]",
    "last_updated": "2025-10-23"
  },
  {
    "id": 81,
    "term": "Greedy Search (Greedy Algorithm)",
    "definition": "A heuristic or algorithmic approach making the locally optimal choice at each step with the intent of finding a global optimum. It doesn’t backtrack or reconsider earlier selections; correctness depends on the problem having the greedy‐choice property.",
    "tags": [
      "algorithm",
      "heuristic",
      "optimization",
      "greedy"
    ],
    "related_terms": [
      "Dynamic Programming",
      "Local Search",
      "Fractional Knapsack",
      "Dijkstra’s Algorithm"
    ],
    "examples": [
      "Coin-change in canonical systems—always pick largest denomination less than remaining amount."
    ],
    "source": "Wikipedia “Greedy algorithm”, GeeksforGeeks",
    "last_updated": "2025-10-23"
  },
  {
    "id": 82,
    "term": "Traveling Salesman Problem (TSP)",
    "definition": "Combinatorial optimization—in an undirected weighted graph with n vertices, find the minimum‐weight Hamiltonian cycle visiting each vertex exactly once and returning to start.",
    "tags": [
      "NP-hard",
      "combinatorial_optimization",
      "graph",
      "benchmark_problem"
    ],
    "related_terms": [
      "Hamiltonian Cycle",
      "NP-hard Problems",
      "Approximation Algorithms",
      "Heuristics",
      "Knapsack Problem"
    ],
    "examples": [
      "TSP is used in planning microchip manufacturing drills, DNA assembly, logistics routing."
    ],
    "source": "Wikipedia, Encyclopaedia Britannica",
    "last_updated": "2025-10-23"
  },
  {
    "id": 83,
    "term": "Graph Search",
    "definition": "A family of algorithms for systematically visiting the vertices and edges of a graph to compute properties such as reachability, shortest paths, or connectivity. Implementations typically track visited nodes to avoid repetition and may be refined into specialized strategies like BFS or DFS.",
    "tags": [
      "graph",
      "search",
      "traversal",
      "algorithm"
    ],
    "related_terms": [
      "Breadth First Search",
      "Depth First Search",
      "Shortest Path",
      "Connectivity"
    ],
    "examples": [
      "A general vertex‐hopping routine iterates through each unvisited vertex, visits it, and recurses or enqueues neighbors."
    ],
    "source": "Wikipedia “Graph traversal”, CMU CS lecture notes",
    "last_updated": "2025-10-23"
  },
  {
    "id": 84,
    "term": "Tabu Search",
    "definition": "A metaheuristic augmenting local search with adaptive memory (tabu list) to avoid revisiting solutions and to escape local optima. It leverages short-term memory and aspiration criteria for strategic exploration.",
    "tags": [
      "metaheuristic",
      "local_search",
      "optimization",
      "combinatorial_optimization"
    ],
    "related_terms": [
      "Local Search",
      "Simulated Annealing",
      "Genetic Algorithms"
    ],
    "examples": [
      "In a scheduling problem, recently swapped tasks are marked “tabu” for a fixed tenure to prevent undoing unless aspiration criteria are met."
    ],
    "source": "Glover et al. (1997), Wikipedia “Tabu Search”",
    "last_updated": "2025-10-23"
  },
  {
    "id": 85,
    "term": "Simulated Annealing",
    "definition": "A probabilistic optimization technique inspired by metallurgy's annealing process. It explores solution spaces by accepting worse solutions with a probability that decreases over time to avoid local optima.",
    "tags": [
      "optimization",
      "metaheuristic",
      "probabilistic_algorithm",
      "search"
    ],
    "related_terms": [
      "Local Search",
      "Metropolis Algorithm",
      "Cooling Schedule",
      "Hill Climbing"
    ],
    "examples": [
      "Temperature T initialized high, neighbor solution S′ accepted if cost lower or with probability exp(–ΔE/T)."
    ],
    "source": "GeeksforGeeks, Cornell Open Textbook",
    "last_updated": "2025-10-25"
  },
  {
    "id": 86,
    "term": "Word Embedding",
    "definition": "A dense, continuous vector representation of words that encodes semantic and syntactic information by placing words with similar meanings close together in a lower-dimensional space, learned from large corpora based on distributional patterns.[ibm.com], [scirp.org]",
    "tags": [
      "Representation Learning",
      "Vector Semantics",
      "NLP",
      "Distributional Hypothesis"
    ],
    "related_terms": [
      "Word2Vec",
      "GloVe",
      "Contextual Embeddings",
      "Distributional Hypothesis"
    ],
    "examples": [],
    "source": "Worth, P. J. (2023). Word Embeddings and Semantic Spaces in Natural Language Processing. International Journal of Intelligence Science, 13(1), 1–21. [scirp.org]",
    "last_updated": "2025-10-29"
  },
  {
    "id": 87,
    "term": "Softmax Function",
    "definition": "A normalized exponential function that converts a vector of real-valued scores (logits) into a probability distribution over multiple classes, with output values between 0 and 1 that sum to 1. [en.wikipedia.org], [geeksforgeeks.org]",
    "tags": [
      "Activation Function",
      "Classification",
      "Probability",
      "Neural Networks"
    ],
    "related_terms": [
      "Cross-Entropy",
      "Logits",
      "Multinomial Logistic Regression",
      "Sigmoid"
    ],
    "examples": [],
    "source": "*“Softmax function”*, Wikipedia, retrieved 2025; also described in GeeksforGeeks (2025). [en.wikipedia.org] [geeksforgeeks.org]",
    "last_updated": "2025-10-29"
  },
  {
    "id": 88,
    "term": "Causal Mask",
    "definition": "A binary mask applied in attention mechanisms to prevent a token from attending to future positions in a sequence; ensures autoregressive (left-to-right) modeling, maintaining causality. [codegenes.net], [geeksforgeeks.org]",
    "tags": [
      "Attention Mechanism",
      "Transformer",
      "Autoregressive",
      "Language Modeling"
    ],
    "related_terms": [
      "Self-Attention",
      "Transformer",
      "Masked Language Modeling",
      "GPT"
    ],
    "examples": [],
    "source": "Codegenes blog tutorial (2025) and GeeksforGeeks (2025) on causal language models. [codegenes.net], [geeksforgeeks.org]",
    "last_updated": "2025-10-29"
  },
  {
    "id": 89,
    "term": "Sequence-to-Sequence Model",
    "definition": "A neural architecture comprising an encoder and a decoder—often RNNs, LSTMs, or Transformers—that maps variable-length input sequences to output sequences, compressed via a context vector (or using attention). [geeksforgeeks.org], [ultralytics.com]",
    "tags": [
      "Encoder–Decoder",
      "Translation",
      "Summarization",
      "Speech-to-Text",
      "NLP"
    ],
    "related_terms": [
      "Encoder",
      "Decoder",
      "Attention Mechanism",
      "Neural Machine Translation",
      "Context Vector"
    ],
    "examples": [],
    "source": "GeeksforGeeks entry (Oct 2025); Ultralytics glossary (2025). [geeksforgeeks.org] [ultralytics.com]",
    "last_updated": "2025-10-29"
  },
  {
    "id": 90,
    "term": "Recurrent Neural Network (RNN)",
    "definition": "A neural network architecture for sequential data where each time-step's output is fed back as input to the next, maintaining a hidden state (“memory”) to model temporal dependencies. [en.wikipedia.org], [geeksforgeeks.org]",
    "tags": [
      "Recurrent Architectures",
      "Sequence Modeling",
      "Time Series",
      "Speech",
      "Text"
    ],
    "related_terms": [
      "LSTM",
      "GRU",
      "Backpropagation Through Time",
      "Encoder–Decoder",
      "Sequence-to-Sequence"
    ],
    "examples": [],
    "source": "*“Recurrent neural network”*, Wikipedia (2025); GeeksforGeeks introduction to RNNs (2025). [en.wikipedia.org] [geeksforgeeks.org]",
    "last_updated": "2025-10-29"
  },
  {
    "id": 91,
    "term": "BLEU (Bilingual Evaluation Understudy)",
    "definition": "An automatic metric for evaluating translation quality, measuring n‑gram precision of machine‑generated text against one or more reference translations, applying a brevity penalty to mitigate overly short outputs. [geeksforgeeks.org], [en.wikipedia.org]",
    "tags": [
      "Evaluation Metric",
      "Machine Translation",
      "NLP"
    ],
    "related_terms": [
      "ROUGE",
      "METEOR",
      "SacreBLEU",
      "N‑gram Overlap"
    ],
    "examples": [],
    "source": "Papineni et al. (2002), “BLEU: a Method for Automatic Evaluation of Machine Translation”; overview on Wikipedia. [en.wikipedia.org]",
    "last_updated": "2025-11-02"
  },
  {
    "id": 92,
    "term": "WMT Translation Task",
    "definition": "The principal shared task of the Conference on Machine Translation (WMT), where participants submit MT systems translating between multiple language pairs across various domains, evaluated via human and automatic metrics. [www2.statmt.org], [aclanthology.org]",
    "tags": [
      "Shared Task",
      "Machine Translation",
      "Evaluation",
      "NLP Benchmark"
    ],
    "related_terms": [
      "BLEU",
      "Human Evaluation",
      "Multilingual MT"
    ],
    "examples": [],
    "source": "Official WMT site and ACL Anthology proceedings. [www2.statmt.org], [aclanthology.org]",
    "last_updated": "2025-11-02"
  },
  {
    "id": 93,
    "term": "Ensembles",
    "definition": "Techniques combining predictions from multiple models—through bagging, boosting, stacking, or voting—to enhance accuracy, reduce variance, and improve generalization. [geeksforgeeks.org], [arxiv.org]",
    "tags": [
      "Model Aggregation",
      "Generalization",
      "Robustness",
      "NLP Methods"
    ],
    "related_terms": [
      "Bagging",
      "Boosting",
      "Stacking",
      "Random Forests"
    ],
    "examples": [],
    "source": "GeeksforGeeks overview (2025); Jia et al. 2023 review on ensemble deep learning in NLP. [geeksforgeeks.org], [arxiv.org]",
    "last_updated": "2025-11-02"
  },
  {
    "id": 94,
    "term": "Constituency Parsing",
    "definition": "The process of analyzing a hierarchical phrase-based syntactic structure of a sentence using context-free grammar (CFG), producing parse trees that represent nested constituents like noun and verb phrases. [numberanalytics.com], [geeksforgeeks.org]",
    "tags": [
      "Syntax",
      "Parsing",
      "Tree-Structure",
      "NLP"
    ],
    "related_terms": [
      "Dependency Parsing",
      "CFG",
      "PCFG",
      "CKY Algorithm"
    ],
    "examples": [],
    "source": "Guide on Constituency Parsing (2025); GeeksforGeeks (2025). [numberanalytics.com], [geeksforgeeks.org]",
    "last_updated": "2025-11-02"
  },
  {
    "id": 95,
    "term": "Context Vector",
    "definition": "A vector based on contextual information from the encoder in sequence-to-sequence models, summarizing input sequences into fixed-length representations for the decoder to generate outputs. Attention mechanisms often enhance context vectors by dynamically weighting encoder hidden states. [stackoverflow.com]",
    "tags": [
      "Encoder–Decoder",
      "Fixed-Length Representation",
      "Attention",
      "Seq2Seq",
      "Neural Machine Translation",
      "Sequence Modeling",
      "Encoders",
      "Attention",
      "Seq2Seq"
    ],
    "related_terms": [
      "Encoder",
      "Decoder",
      "Hidden State",
      "Attention Mechanism",
      "Encoder Hidden States",
      "Attention Weights / Alignment Scores",
      "Decoder Hidden State",
      "Attention Mechanism (Bahdanau",
      "Luong)"
    ],
    "examples": [],
    "source": "Stack Overflow explanation (2023). [stackoverflow.com]",
    "last_updated": "2025-11-02"
  },
  {
    "id": 96,
    "term": "Recurrence",
    "definition": "A mechanism in recurrent neural networks (RNNs) where each hidden state depends on both the current input and the previous hidden state, enabling modeling of temporal dependencies in sequence data. [geeksforgeeks.org], [en.wikipedia.org]",
    "tags": [
      "Sequential Modeling",
      "RNN",
      "Time-Series",
      "NLP"
    ],
    "related_terms": [
      "RNN",
      "LSTM",
      "GRU",
      "Backpropagation Through Time",
      "Polynomial Time"
    ],
    "examples": [],
    "source": "GeeksforGeeks (2025); Wikipedia entry on RNNs. [geeksforgeeks.org], [en.wikipedia.org]",
    "last_updated": "2025-11-02"
  },
  {
    "id": 97,
    "term": "Convolutions",
    "definition": "Local feature extractors applying filters over word sequences to capture n‑gram features; originally common in CNN-based NLP, but in transformers, they're replaced—or augmented—by self-attention layers that capture global dependencies. [coursera.org], [geeksforgeeks.org]",
    "tags": [
      "CNN",
      "Feature Extraction",
      "Local Patterns",
      "NLP"
    ],
    "related_terms": [
      "CNN",
      "Self-Attention",
      "Transformer",
      "RNN"
    ],
    "examples": [],
    "source": "Coursera (2025); GeeksforGeeks overview (2025). [coursera.org], [geeksforgeeks.org]",
    "last_updated": "2025-11-02"
  },
  {
    "id": 98,
    "term": "Transformer",
    "definition": "A neural architecture built on multi-head self-attention and feed-forward layers (no recurrence), enabling parallel sequence processing and capturing long-range dependencies; introduced in “Attention Is All You Need” (2017). [en.wikipedia.org], [arxiv.org]",
    "tags": [
      "Self-Attention",
      "Encoder–Decoder",
      "NLP",
      "Parallelism"
    ],
    "related_terms": [
      "Multi-Head Attention",
      "Positional Encoding",
      "BERT",
      "GPT"
    ],
    "examples": [],
    "source": "Wikipedia; Tong Xiao & Jingbo Zhu (2023) arXiv survey. [en.wikipedia.org], [arxiv.org]",
    "last_updated": "2025-11-02"
  },
  {
    "id": 99,
    "term": "Self-Attention",
    "definition": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It allows the model to weigh the importance of different words in a sequence when encoding a particular word, capturing long-range dependencies effectively. [en.wikipedia.org], [geeksforgeeks.org]",
    "tags": [
      "Attention",
      "Milti-head Attention"
    ],
    "related_terms": [
      "Attention",
      "Multi-head Attention",
      "neural network",
      "transformer"
    ],
    "examples": [],
    "source": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. (2017), *\"Attention is All You Need\"*, Google, In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17).",
    "last_updated": "2025-11-02"
  }
]
