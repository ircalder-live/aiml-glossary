{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1962c7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root set to: /home/ian/dev/aiml-glossary\n",
      "Working directory now: /home/ian/dev/aiml-glossary\n",
      "âœ… Python executable: /home/ian/miniforge3/envs/glossary/bin/python\n",
      "data exists: True -> /home/ian/dev/aiml-glossary/data\n",
      "visualizations exists: True -> /home/ian/dev/aiml-glossary/visualizations\n",
      "src exists: True -> /home/ian/dev/aiml-glossary/src\n",
      "âœ… Core modules imported successfully\n",
      "Glossary file exists: True -> /home/ian/dev/aiml-glossary/data/aiml_glossary.json\n",
      "âœ… Glossary JSON parsed, 98 entries\n",
      "âœ… MLflow run started and logged successfully\n"
     ]
    }
   ],
   "source": [
    "# %% Smoke test + bootstrap + MLflow\n",
    "\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Bootstrap: set repo root for imports and paths ---\n",
    "repo_root = Path.cwd().parent\n",
    "os.chdir(repo_root)\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(\"Repo root set to:\", repo_root)\n",
    "print(\"Working directory now:\", Path.cwd())\n",
    "print(\"âœ… Python executable:\", sys.executable)\n",
    "\n",
    "# --- Directory checks ---\n",
    "for d in [\"data\", \"visualizations\", \"src\"]:\n",
    "    p = repo_root / d\n",
    "    if not p.exists():\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"ðŸ“‚ Created missing directory: {p}\")\n",
    "    else:\n",
    "        print(f\"{d} exists:\", p.exists(), \"->\", p)\n",
    "\n",
    "# --- Module imports ---\n",
    "try:\n",
    "    import src.generate_outputs\n",
    "    import src.cluster_analysis\n",
    "    import src.semantic_clustering\n",
    "    import src.evaluate_clusters\n",
    "    print(\"âœ… Core modules imported successfully\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Import failure:\", e)\n",
    "\n",
    "# --- Glossary file check ---\n",
    "glossary_file = repo_root / \"data/aiml_glossary.json\"\n",
    "print(\"Glossary file exists:\", glossary_file.exists(), \"->\", glossary_file)\n",
    "if glossary_file.exists():\n",
    "    try:\n",
    "        with open(glossary_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            glossary_dict = json.load(f)\n",
    "        print(f\"âœ… Glossary JSON parsed, {len(glossary_dict)} entries\")\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Glossary JSON parse error:\", e)\n",
    "\n",
    "# --- MLflow smoke test ---\n",
    "try:\n",
    "    import mlflow\n",
    "    with mlflow.start_run(run_name=\"smoke_test\", nested=True):\n",
    "        mlflow.log_param(\"smoke_test\", True)\n",
    "        mlflow.log_metric(\"smoke_metric\", 1.0)\n",
    "    print(\"âœ… MLflow run started and logged successfully\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ MLflow smoke test failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca0b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path.cwd()  # bootstrap already set cwd to repo root\n",
    "sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "def resolve_uri(uri: str) -> Path:\n",
    "    prefix, name = uri.split(\":\", 1)\n",
    "    if prefix == \"data\":\n",
    "        return REPO_ROOT / \"data\" / name\n",
    "    elif prefix == \"output\":\n",
    "        return REPO_ROOT / \"output\" / name\n",
    "    elif prefix == \"visualizations\":\n",
    "        return REPO_ROOT / \"visualizations\" / name\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown URI prefix: {prefix}\")\n",
    "\n",
    "from src.generate_outputs import generate\n",
    "from src.cluster_analysis import run_clustering\n",
    "from src.semantic_clustering import run_semantic_clustering\n",
    "from src.evaluate_clusters import evaluate_clusters\n",
    "from src.enrich_glossary import enrich_glossary\n",
    "from src.link_dictionary import build_link_dictionary\n",
    "from src.coverage_report import generate_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227e0417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Outputs generated: /home/ian/dev/aiml-glossary/output/terms.csv, /home/ian/dev/aiml-glossary/output/glossary_copy.json\n"
     ]
    }
   ],
   "source": [
    "# %% Step 1: Generate outputs\n",
    "generate(\"data/aiml_glossary.json\", \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f751fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Link dictionary built: /home/ian/dev/aiml-glossary/link_dictionary.json\n",
      "ðŸ“Š Link dictionary logged to MLflow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Supervised Learning': [],\n",
       " 'Unsupervised Learning': [],\n",
       " 'Classification': ['Supervised Learning'],\n",
       " 'Regression': ['Supervised Learning'],\n",
       " 'Clustering': ['Supervised Learning', 'Unsupervised Learning'],\n",
       " 'Data Cleaning': [],\n",
       " 'Feature Scaling': [],\n",
       " 'Dimensionality Reduction': [],\n",
       " 'Encoding Categorical Variables': [],\n",
       " 'Feature Extraction': [],\n",
       " 'Cross-Validation': ['Generalization'],\n",
       " 'Confusion Matrix': ['Classification'],\n",
       " 'Precision': [],\n",
       " 'Recall': [],\n",
       " 'F1 Score': ['Classification', 'Precision', 'Recall'],\n",
       " 'Hyperparameter Tuning': [],\n",
       " 'Grid Search': [],\n",
       " 'Random Search': [],\n",
       " 'Bayesian Optimization': [],\n",
       " 'Early Stopping': [],\n",
       " 'Artificial Neural Network': [],\n",
       " 'Activation Function': [],\n",
       " 'Backpropagation': [],\n",
       " 'Convolutional Neural Network': [],\n",
       " 'Recurrent Neural Network': [],\n",
       " 'Model Deployment': [],\n",
       " 'Model Serving': [],\n",
       " 'Monitoring': [],\n",
       " 'A/B Testing': [],\n",
       " 'Rollback': [],\n",
       " 'Concept Drift': [],\n",
       " 'Temporal Dependency': [],\n",
       " 'Replay Methods': ['Continual Learning'],\n",
       " 'Regularization Methods': ['Continual Learning'],\n",
       " 'Isolation Methods': [],\n",
       " 'Symbolic Regression': ['Regression'],\n",
       " 'Genetic Programming': [],\n",
       " 'Randomized Algorithms': [],\n",
       " 'Computational Complexity': [],\n",
       " 'Symbolic AI': [],\n",
       " 'Skipgram': ['Word2Vec', 'Word Embedding'],\n",
       " 'Word2Vec': [],\n",
       " 'Distribution Hypothesis': ['Word Embedding'],\n",
       " 'Text Representation': ['Word2Vec'],\n",
       " 'Word Representation': [],\n",
       " 'Simulated Annealing': [],\n",
       " 'Monte Carlo Algorithm': [],\n",
       " 'Las Vegas Algorithm': [],\n",
       " 'Pseudocode': [],\n",
       " 'Continual Learning': [],\n",
       " 'Random Forest': ['Classification',\n",
       "  'Regression',\n",
       "  'Decision Tree',\n",
       "  'Ensemble Learning',\n",
       "  'Overfitting',\n",
       "  'Generalization'],\n",
       " 'Adaptive Random Forest (ARF)': ['Concept Drift',\n",
       "  'Random Forest',\n",
       "  'Decision Tree',\n",
       "  'Ensemble Learning'],\n",
       " 'Ensembles': ['Generalization', 'Generalization'],\n",
       " 'Naive Bayes': ['Classification'],\n",
       " 'Decision Tree': ['Supervised Learning', 'Classification', 'Regression'],\n",
       " 'k-Nearest Neighbors (k-NN)': ['Classification', 'Regression'],\n",
       " 'Adaptive Machine Learning (Adaptive ML)': ['Concept Drift'],\n",
       " 'Explainable AI (XAI)': [],\n",
       " 'Bootstrap Aggregating (Bagging)': ['Decision Tree',\n",
       "  'Ensemble Learning',\n",
       "  'Overfitting'],\n",
       " 'Ensemble Learning': ['Ensembles', 'Generalization', 'Ensembles'],\n",
       " 'Tournament Selection': [],\n",
       " 'Parsimony Pressure': ['Genetic Programming', 'Bloat', 'Fitness Function'],\n",
       " 'Bloat': ['Genetic Programming', 'Mutation', 'Crossover', 'Expression Tree'],\n",
       " 'Underfitting': [],\n",
       " 'Overfitting': ['Generalization'],\n",
       " 'Bias-Variance Trade-off': [],\n",
       " 'Generalization': ['Supervised Learning', 'Overfitting'],\n",
       " 'Mutation': [],\n",
       " 'Crossover': [],\n",
       " 'Lexicase Selection': [],\n",
       " 'Fitness Function': [],\n",
       " 'Expression Tree': ['Regression', 'Symbolic Regression'],\n",
       " 'Monte Carlo Tree Search (MCTS)': ['Backpropagation'],\n",
       " 'Bayesian Symbolic Regression': ['Regression',\n",
       "  'Symbolic Regression',\n",
       "  'Expression Tree'],\n",
       " 'Serviceâ€‘oriented Computing (SOC)': [],\n",
       " 'Implicit Graphs': [],\n",
       " 'Depth First Search (DFS)': [],\n",
       " 'Breadth First Search (BFS)': [],\n",
       " 'Graph Connectivity': [],\n",
       " 'Greedy Search (Greedy Algorithm)': [],\n",
       " 'Traveling Salesman Problem (TSP)': [],\n",
       " 'Graph Search': [],\n",
       " 'Tabu Search': [],\n",
       " 'Word Embedding': [],\n",
       " 'Softmax Function': [],\n",
       " 'Causal Mask': [],\n",
       " 'Sequence-to-Sequence Model': ['Context Vector', 'Transformer'],\n",
       " 'Recurrent Neural Network (RNN)': [],\n",
       " 'BLEU (Bilingual Evaluation Understudy)': ['Precision'],\n",
       " 'WMT Translation Task': [],\n",
       " 'Constituency Parsing': [],\n",
       " 'Context Vector': ['Sequence-to-Sequence Model'],\n",
       " 'Recurrence': ['Recurrent Neural Network'],\n",
       " 'Convolutions': ['Transformer', 'Self-Attention'],\n",
       " 'Transformer': ['Recurrence', 'Self-Attention'],\n",
       " 'Self-Attention': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Step 2: Build link dictionary\n",
    "build_link_dictionary(\"data/aiml_glossary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8931f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enriched glossary saved: /home/ian/dev/aiml-glossary/data/enriched_glossary.json\n",
      "ðŸ“Š Enriched glossary logged to MLflow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'supervised-learning': {'id': 1,\n",
       "  'term': 'Supervised Learning',\n",
       "  'definition': 'A type of machine learning where the model is trained on labeled data to learn a mapping from inputs to outputs.',\n",
       "  'tags': ['machine learning', 'labeled data', 'prediction'],\n",
       "  'related_terms': ['Classification', 'Regression'],\n",
       "  'examples': ['Predicting house prices using historical sales data.'],\n",
       "  'source': 'Scikit-learn documentation',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'supervised-learning',\n",
       "  'original_term': 'Supervised Learning',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'unsupervised-learning': {'id': 2,\n",
       "  'term': 'Unsupervised Learning',\n",
       "  'definition': 'A machine learning approach where the model identifies patterns in data without labeled outcomes.',\n",
       "  'tags': ['clustering', 'dimensionality reduction', 'pattern discovery'],\n",
       "  'related_terms': ['Clustering', 'Principal Component Analysis'],\n",
       "  'examples': ['Segmenting customers based on purchasing behavior.'],\n",
       "  'source': 'Hastie, Tibshirani & Friedman (2009). The Elements of Statistical Learning.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'unsupervised-learning',\n",
       "  'original_term': 'Unsupervised Learning',\n",
       "  'linked_terms': []},\n",
       " 'classification': {'id': 3,\n",
       "  'term': 'Classification',\n",
       "  'definition': 'A supervised learning task where the goal is to assign discrete labels to input data.',\n",
       "  'tags': ['supervised learning', 'discrete output', 'prediction'],\n",
       "  'related_terms': ['Logistic Regression', 'Decision Tree'],\n",
       "  'examples': ['Email spam detection, disease diagnosis.'],\n",
       "  'source': 'Bishop, C. M. (2006). Pattern Recognition and Machine Learning.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'classification',\n",
       "  'original_term': 'Classification',\n",
       "  'linked_terms': ['Supervised Learning']},\n",
       " 'regression': {'id': 4,\n",
       "  'term': 'Regression',\n",
       "  'definition': 'A supervised learning task where the goal is to predict continuous numerical values.',\n",
       "  'tags': ['supervised learning', 'continuous output', 'prediction'],\n",
       "  'related_terms': ['Linear Regression', 'Polynomial Regression'],\n",
       "  'examples': ['Predicting temperature based on historical weather data.'],\n",
       "  'source': 'James et al. (2013). An Introduction to Statistical Learning.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'regression',\n",
       "  'original_term': 'Regression',\n",
       "  'linked_terms': ['Supervised Learning']},\n",
       " 'clustering': {'id': 5,\n",
       "  'term': 'Clustering',\n",
       "  'definition': 'An unsupervised learning technique that groups data points into clusters based on similarity.',\n",
       "  'tags': ['unsupervised learning', 'grouping', 'similarity'],\n",
       "  'related_terms': ['K-Means', 'Hierarchical Clustering'],\n",
       "  'examples': ['Grouping news articles by topic.'],\n",
       "  'source': 'Jain, A. K. (2010). Data clustering: 50 years beyond K-means.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'clustering',\n",
       "  'original_term': 'Clustering',\n",
       "  'linked_terms': ['Supervised Learning', 'Unsupervised Learning']},\n",
       " 'data-cleaning': {'id': 6,\n",
       "  'term': 'Data Cleaning',\n",
       "  'definition': 'The process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset to improve data quality.',\n",
       "  'tags': ['data preprocessing', 'quality assurance', 'missing values'],\n",
       "  'related_terms': ['Outlier Detection', 'Missing Value Imputation'],\n",
       "  'examples': ['Removing duplicate rows, handling missing values, correcting typos in categorical data.'],\n",
       "  'source': \"Kelleher, J.D., Namee, B., & D'Arcy, A. (2015). Fundamentals of Machine Learning for Predictive Data Analytics.\",\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'data-cleaning',\n",
       "  'original_term': 'Data Cleaning',\n",
       "  'linked_terms': []},\n",
       " 'feature-scaling': {'id': 7,\n",
       "  'term': 'Feature Scaling',\n",
       "  'definition': 'A technique used to normalize the range of independent variables or features of data to ensure uniformity and improve model performance.',\n",
       "  'tags': ['normalization', 'standardization', 'preprocessing'],\n",
       "  'related_terms': ['Min-Max Scaling', 'StandardScaler'],\n",
       "  'examples': ['Scaling features to a range of [0,1] using Min-Max normalization.'],\n",
       "  'source': 'GÃ©ron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'feature-scaling',\n",
       "  'original_term': 'Feature Scaling',\n",
       "  'linked_terms': []},\n",
       " 'dimensionality-reduction': {'id': 8,\n",
       "  'term': 'Dimensionality Reduction',\n",
       "  'definition': 'The process of reducing the number of input variables in a dataset to simplify models and reduce computational cost.',\n",
       "  'tags': ['feature engineering', 'PCA', 'data compression'],\n",
       "  'related_terms': ['Principal Component Analysis', 't-SNE'],\n",
       "  'examples': ['Using PCA to reduce a dataset from 100 features to 10 principal components.'],\n",
       "  'source': 'Jolliffe, I.T., & Cadima, J. (2016). Principal component analysis: a review and recent developments.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'dimensionality-reduction',\n",
       "  'original_term': 'Dimensionality Reduction',\n",
       "  'linked_terms': []},\n",
       " 'encoding-categorical-variables': {'id': 9,\n",
       "  'term': 'Encoding Categorical Variables',\n",
       "  'definition': 'The process of converting categorical data into numerical format so that it can be used in machine learning models.',\n",
       "  'tags': ['categorical data', 'preprocessing', 'encoding'],\n",
       "  'related_terms': ['One-Hot Encoding', 'Label Encoding'],\n",
       "  'examples': [\"Transforming 'Color' feature with values ['Red', 'Blue', 'Green'] into one-hot encoded vectors.\"],\n",
       "  'source': 'Pedregosa et al. (2011). Scikit-learn: Machine Learning in Python.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'encoding-categorical-variables',\n",
       "  'original_term': 'Encoding Categorical Variables',\n",
       "  'linked_terms': []},\n",
       " 'feature-extraction': {'id': 10,\n",
       "  'term': 'Feature Extraction',\n",
       "  'definition': 'The process of transforming raw data into informative features that can be used for model training.',\n",
       "  'tags': ['feature engineering', 'preprocessing', 'transformation'],\n",
       "  'related_terms': ['Feature Selection', 'Dimensionality Reduction'],\n",
       "  'examples': ['Extracting edge features from images using Sobel filters.'],\n",
       "  'source': 'Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'feature-extraction',\n",
       "  'original_term': 'Feature Extraction',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'cross-validation': {'id': 11,\n",
       "  'term': 'Cross-Validation',\n",
       "  'definition': 'A model evaluation technique that partitions the dataset into multiple subsets, training the model on some and validating it on others to assess generalization performance.',\n",
       "  'tags': ['model evaluation', 'validation', 'resampling'],\n",
       "  'related_terms': ['Train-Test Split', 'K-Fold Cross-Validation'],\n",
       "  'examples': [\"Using 5-fold cross-validation to evaluate a classifier's accuracy across different data splits.\"],\n",
       "  'source': 'Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'cross-validation',\n",
       "  'original_term': 'Cross-Validation',\n",
       "  'linked_terms': ['Artificial Intelligence', 'Generalization']},\n",
       " 'confusion-matrix': {'id': 12,\n",
       "  'term': 'Confusion Matrix',\n",
       "  'definition': 'A table used to describe the performance of a classification model by showing the counts of true positives, false positives, true negatives, and false negatives.',\n",
       "  'tags': ['classification', 'evaluation', 'metrics'],\n",
       "  'related_terms': ['Precision', 'Recall', 'F1 Score'],\n",
       "  'examples': ['A binary classifier predicting spam emails yields a confusion matrix with 80 true positives, 10 false positives, 5 false negatives, and 105 true negatives.'],\n",
       "  'source': 'Powers, D.M.W. (2011). Evaluation: From precision, recall and F-measure to ROC, informedness, markedness and correlation.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'confusion-matrix',\n",
       "  'original_term': 'Confusion Matrix',\n",
       "  'linked_terms': ['Classification']},\n",
       " 'precision': {'id': 13,\n",
       "  'term': 'Precision',\n",
       "  'definition': 'The ratio of true positive predictions to the total number of positive predictions made by the model, indicating the accuracy of positive predictions.',\n",
       "  'tags': ['classification', 'metrics', 'evaluation'],\n",
       "  'related_terms': ['Recall', 'F1 Score', 'Confusion Matrix'],\n",
       "  'examples': ['If a model predicts 100 positive cases and 80 of them are correct, the precision is 0.80.'],\n",
       "  'source': 'Sokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'precision',\n",
       "  'original_term': 'Precision',\n",
       "  'linked_terms': []},\n",
       " 'recall': {'id': 14,\n",
       "  'term': 'Recall',\n",
       "  'definition': \"The ratio of true positive predictions to the total number of actual positive instances, measuring the model's ability to identify positive cases.\",\n",
       "  'tags': ['classification', 'metrics', 'evaluation'],\n",
       "  'related_terms': ['Precision', 'F1 Score', 'Confusion Matrix'],\n",
       "  'examples': ['If there are 100 actual positive cases and the model correctly identifies 80, the recall is 0.80.'],\n",
       "  'source': 'Sokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'recall',\n",
       "  'original_term': 'Recall',\n",
       "  'linked_terms': []},\n",
       " 'f1-score': {'id': 15,\n",
       "  'term': 'F1 Score',\n",
       "  'definition': 'The harmonic mean of precision and recall, providing a single metric that balances both concerns in classification tasks.',\n",
       "  'tags': ['classification', 'metrics', 'evaluation'],\n",
       "  'related_terms': ['Precision', 'Recall', 'Confusion Matrix'],\n",
       "  'examples': ['If a model has precision 0.80 and recall 0.70, the F1 score is approximately 0.746.'],\n",
       "  'source': 'Powers, D.M.W. (2011). Evaluation: From precision, recall and F-measure to ROC, informedness, markedness and correlation.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'f1-score',\n",
       "  'original_term': 'F1 Score',\n",
       "  'linked_terms': ['Classification', 'Precision', 'Recall']},\n",
       " 'hyperparameter-tuning': {'id': 16,\n",
       "  'term': 'Hyperparameter Tuning',\n",
       "  'definition': 'The process of optimizing the configuration parameters that are not learned during training but affect model performance.',\n",
       "  'tags': ['optimization', 'model tuning', 'hyperparameters'],\n",
       "  'related_terms': ['Grid Search', 'Random Search', 'Bayesian Optimization'],\n",
       "  'examples': ['Adjusting the learning rate and number of layers in a neural network to improve accuracy.'],\n",
       "  'source': 'Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'hyperparameter-tuning',\n",
       "  'original_term': 'Hyperparameter Tuning',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'grid-search': {'id': 17,\n",
       "  'term': 'Grid Search',\n",
       "  'definition': 'An exhaustive search method that evaluates all possible combinations of hyperparameter values in a specified grid.',\n",
       "  'tags': ['hyperparameter tuning', 'grid search', 'model selection'],\n",
       "  'related_terms': ['Random Search', 'Cross-Validation'],\n",
       "  'examples': ['Testing combinations of kernel types and regularization parameters in an SVM.'],\n",
       "  'source': 'Pedregosa et al. (2011). Scikit-learn: Machine Learning in Python.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'grid-search',\n",
       "  'original_term': 'Grid Search',\n",
       "  'linked_terms': []},\n",
       " 'random-search': {'id': 18,\n",
       "  'term': 'Random Search',\n",
       "  'definition': 'A hyperparameter optimization technique that samples random combinations of parameters from a predefined distribution.',\n",
       "  'tags': ['hyperparameter tuning', 'random sampling', 'optimization'],\n",
       "  'related_terms': ['Grid Search', 'Bayesian Optimization'],\n",
       "  'examples': ['Randomly selecting values for dropout rate and batch size to find optimal settings.'],\n",
       "  'source': 'Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'random-search',\n",
       "  'original_term': 'Random Search',\n",
       "  'linked_terms': []},\n",
       " 'bayesian-optimization': {'id': 19,\n",
       "  'term': 'Bayesian Optimization',\n",
       "  'definition': 'An optimization technique that builds a probabilistic model of the objective function and uses it to select the most promising hyperparameters.',\n",
       "  'tags': ['hyperparameter tuning', 'bayesian methods', 'optimization'],\n",
       "  'related_terms': ['Gaussian Process', 'Acquisition Function'],\n",
       "  'examples': ['Using Bayesian optimization to tune the learning rate and momentum in a neural network.'],\n",
       "  'source': 'Snoek, J., Larochelle, H., & Adams, R.P. (2012). Practical Bayesian optimization of machine learning algorithms.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'bayesian-optimization',\n",
       "  'original_term': 'Bayesian Optimization',\n",
       "  'linked_terms': []},\n",
       " 'early-stopping': {'id': 20,\n",
       "  'term': 'Early Stopping',\n",
       "  'definition': \"A regularization technique that stops training when the model's performance on a validation set starts to degrade.\",\n",
       "  'tags': ['regularization', 'model tuning', 'overfitting prevention'],\n",
       "  'related_terms': ['Validation Set', 'Overfitting'],\n",
       "  'examples': ['Halting training after 10 epochs when validation loss begins to increase.'],\n",
       "  'source': 'Prechelt, L. (1998). Early stoppingâ€”but when? In Neural Networks: Tricks of the Trade.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'early-stopping',\n",
       "  'original_term': 'Early Stopping',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'artificial-neural-network': {'id': 21,\n",
       "  'term': 'Artificial Neural Network',\n",
       "  'definition': 'A computational model inspired by the structure and function of biological neural networks, consisting of interconnected nodes (neurons) organized in layers.',\n",
       "  'tags': ['deep learning', 'neural networks', 'machine learning'],\n",
       "  'related_terms': ['Perceptron', 'Multilayer Perceptron'],\n",
       "  'examples': ['Using an ANN to classify handwritten digits in the MNIST dataset.'],\n",
       "  'source': 'Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'artificial-neural-network',\n",
       "  'original_term': 'Artificial Neural Network',\n",
       "  'linked_terms': ['Neural Network']},\n",
       " 'activation-function': {'id': 22,\n",
       "  'term': 'Activation Function',\n",
       "  'definition': \"A mathematical function applied to a neuron's output to introduce non-linearity into the model, enabling learning of complex patterns.\",\n",
       "  'tags': ['neural networks', 'non-linearity', 'deep learning'],\n",
       "  'related_terms': ['ReLU', 'Sigmoid', 'Tanh'],\n",
       "  'examples': ['Using ReLU activation in hidden layers of a CNN.'],\n",
       "  'source': 'Nwankpa, C., et al. (2018). Activation Functions: Comparison and Usage.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'activation-function',\n",
       "  'original_term': 'Activation Function',\n",
       "  'linked_terms': []},\n",
       " 'backpropagation': {'id': 23,\n",
       "  'term': 'Backpropagation',\n",
       "  'definition': 'An algorithm used to train neural networks by computing gradients of the loss function and updating weights via gradient descent.',\n",
       "  'tags': ['training', 'optimization', 'neural networks'],\n",
       "  'related_terms': ['Gradient Descent', 'Loss Function'],\n",
       "  'examples': ['Training a neural network using backpropagation with stochastic gradient descent.'],\n",
       "  'source': 'Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986). Learning representations by back-propagating errors.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'backpropagation',\n",
       "  'original_term': 'Backpropagation',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'convolutional-neural-network': {'id': 24,\n",
       "  'term': 'Convolutional Neural Network',\n",
       "  'definition': 'A type of deep neural network designed to process structured grid data such as images, using convolutional layers to extract spatial features.',\n",
       "  'tags': ['deep learning', 'computer vision', 'CNN'],\n",
       "  'related_terms': ['Pooling', 'Feature Maps'],\n",
       "  'examples': ['Using a CNN to classify images in the CIFAR-10 dataset.'],\n",
       "  'source': 'LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'convolutional-neural-network',\n",
       "  'original_term': 'Convolutional Neural Network',\n",
       "  'linked_terms': []},\n",
       " 'recurrent-neural-network': {'id': 25,\n",
       "  'term': 'Recurrent Neural Network',\n",
       "  'definition': 'A class of neural networks where connections between nodes form directed cycles, allowing modeling of sequential data.',\n",
       "  'tags': ['sequence modeling', 'RNN', 'deep learning'],\n",
       "  'related_terms': ['LSTM', 'GRU'],\n",
       "  'examples': ['Using an RNN to predict the next word in a sentence.'],\n",
       "  'source': 'Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'recurrent-neural-network',\n",
       "  'original_term': 'Recurrent Neural Network',\n",
       "  'linked_terms': ['Neural Network']},\n",
       " 'model-deployment': {'id': 26,\n",
       "  'term': 'Model Deployment',\n",
       "  'definition': 'The process of integrating a trained machine learning model into a production environment where it can make predictions on real-world data.',\n",
       "  'tags': ['deployment', 'production', 'ML lifecycle'],\n",
       "  'related_terms': ['Model Serving', 'API'],\n",
       "  'examples': ['Deploying a fraud detection model as a REST API.'],\n",
       "  'source': 'Sculley, D., et al. (2015). Hidden Technical Debt in Machine Learning Systems.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'model-deployment',\n",
       "  'original_term': 'Model Deployment',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'model-serving': {'id': 27,\n",
       "  'term': 'Model Serving',\n",
       "  'definition': 'The infrastructure and tools used to host and deliver machine learning models for inference in production environments.',\n",
       "  'tags': ['deployment', 'inference', 'scalability'],\n",
       "  'related_terms': ['Model Deployment', 'REST API'],\n",
       "  'examples': ['Using TensorFlow Serving to host a trained model.'],\n",
       "  'source': 'TensorFlow Serving documentation.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'model-serving',\n",
       "  'original_term': 'Model Serving',\n",
       "  'linked_terms': []},\n",
       " 'monitoring': {'id': 28,\n",
       "  'term': 'Monitoring',\n",
       "  'definition': 'The practice of tracking model performance and system metrics in production to detect issues such as data drift or latency.',\n",
       "  'tags': ['observability', 'performance', 'ML operations'],\n",
       "  'related_terms': ['Concept Drift', 'Logging'],\n",
       "  'examples': ['Monitoring prediction accuracy and response time of a deployed model.'],\n",
       "  'source': 'Baylor, D., et al. (2017). TFX: A TensorFlow-Based Production-Scale Machine Learning Platform.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'monitoring',\n",
       "  'original_term': 'Monitoring',\n",
       "  'linked_terms': []},\n",
       " 'ab-testing': {'id': 29,\n",
       "  'term': 'A/B Testing',\n",
       "  'definition': 'An experimental technique to compare two versions of a model or system by exposing users to each and measuring performance differences.',\n",
       "  'tags': ['experimentation', 'evaluation', 'deployment'],\n",
       "  'related_terms': ['Model Comparison', 'Online Testing'],\n",
       "  'examples': ['Testing two recommendation models to determine which yields higher click-through rates.'],\n",
       "  'source': 'Kohavi, R., et al. (2009). Controlled experiments on the web: survey and practical guide.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'ab-testing',\n",
       "  'original_term': 'A/B Testing',\n",
       "  'linked_terms': []},\n",
       " 'rollback': {'id': 30,\n",
       "  'term': 'Rollback',\n",
       "  'definition': 'The process of reverting to a previous model version or system state in response to performance degradation or failure.',\n",
       "  'tags': ['deployment', 'reliability', 'version control'],\n",
       "  'related_terms': ['Model Versioning', 'Monitoring'],\n",
       "  'examples': ['Rolling back to a previous model after detecting a drop in accuracy.'],\n",
       "  'source': 'Sculley, D., et al. (2015). Hidden Technical Debt in Machine Learning Systems.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'rollback',\n",
       "  'original_term': 'Rollback',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'concept-drift': {'id': 31,\n",
       "  'term': 'Concept Drift',\n",
       "  'definition': 'A phenomenon where the statistical properties of the target variable change over time, affecting model performance.',\n",
       "  'tags': ['stream learning', 'non-stationarity', 'adaptation'],\n",
       "  'related_terms': ['Data Drift', 'Model Monitoring'],\n",
       "  'examples': ['Customer behavior changes over time, impacting churn prediction models.'],\n",
       "  'source': 'Gama, J., et al. (2014). A survey on concept drift adaptation.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'concept-drift',\n",
       "  'original_term': 'Concept Drift',\n",
       "  'linked_terms': []},\n",
       " 'temporal-dependency': {'id': 32,\n",
       "  'term': 'Temporal Dependency',\n",
       "  'definition': 'The relationship between data points across time, where past values influence future outcomes.',\n",
       "  'tags': ['sequence modeling', 'time series', 'stream learning'],\n",
       "  'related_terms': ['Autocorrelation', 'Lag Features'],\n",
       "  'examples': [\"Stock prices influenced by previous days' values.\"],\n",
       "  'source': 'Hamilton, J.D. (1994). Time Series Analysis.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'temporal-dependency',\n",
       "  'original_term': 'Temporal Dependency',\n",
       "  'linked_terms': []},\n",
       " 'replay-methods': {'id': 33,\n",
       "  'term': 'Replay Methods',\n",
       "  'definition': 'Techniques in continual learning that store and replay past data to mitigate forgetting and improve stability.',\n",
       "  'tags': ['continual learning', 'memory', 'rehearsal'],\n",
       "  'related_terms': ['Experience Replay', 'Episodic Memory'],\n",
       "  'examples': ['Using a buffer of past samples to retrain a model incrementally.'],\n",
       "  'source': 'Rolnick, D., et al. (2019). Experience Replay for Continual Learning.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'replay-methods',\n",
       "  'original_term': 'Replay Methods',\n",
       "  'linked_terms': ['Continual Learning']},\n",
       " 'regularization-methods': {'id': 34,\n",
       "  'term': 'Regularization Methods',\n",
       "  'definition': 'Approaches that constrain model updates to preserve previously learned knowledge during continual learning.',\n",
       "  'tags': ['continual learning', 'stability', 'regularization'],\n",
       "  'related_terms': ['Elastic Weight Consolidation', 'L2 Regularization'],\n",
       "  'examples': ['Using EWC to prevent catastrophic forgetting in neural networks.'],\n",
       "  'source': 'Kirkpatrick, J., et al. (2017). Overcoming catastrophic forgetting in neural networks.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'regularization-methods',\n",
       "  'original_term': 'Regularization Methods',\n",
       "  'linked_terms': ['Artificial Intelligence', 'Continual Learning']},\n",
       " 'isolation-methods': {'id': 35,\n",
       "  'term': 'Isolation Methods',\n",
       "  'definition': 'Strategies that allocate separate model components or parameters for different tasks to avoid interference.',\n",
       "  'tags': ['continual learning', 'modularity', 'task separation'],\n",
       "  'related_terms': ['Progressive Networks', 'Dynamic Architectures'],\n",
       "  'examples': ['Using task-specific subnetworks to isolate learning.'],\n",
       "  'source': 'Rusu, A.A., et al. (2016). Progressive neural networks.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'isolation-methods',\n",
       "  'original_term': 'Isolation Methods',\n",
       "  'linked_terms': []},\n",
       " 'symbolic-regression': {'id': 36,\n",
       "  'term': 'Symbolic Regression',\n",
       "  'definition': 'A type of regression analysis that searches for mathematical expressions that best fit a dataset using symbolic representations.',\n",
       "  'tags': ['symbolic AI', 'regression', 'genetic programming'],\n",
       "  'related_terms': ['Genetic Programming', 'Expression Trees'],\n",
       "  'examples': ['Using symbolic regression to discover physical laws from experimental data.'],\n",
       "  'source': 'Koza, J.R. (1992). Genetic Programming: On the Programming of Computers by Means of Natural Selection.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'symbolic-regression',\n",
       "  'original_term': 'Symbolic Regression',\n",
       "  'linked_terms': ['Regression']},\n",
       " 'genetic-programming': {'id': 37,\n",
       "  'term': 'Genetic Programming',\n",
       "  'definition': 'An evolutionary algorithm-based methodology that evolves computer programs to solve problems by mimicking natural selection.',\n",
       "  'tags': ['evolutionary algorithms', 'symbolic AI', 'optimization'],\n",
       "  'related_terms': ['Genetic Algorithms', 'Symbolic Regression'],\n",
       "  'examples': ['Evolving a sorting algorithm using genetic programming.'],\n",
       "  'source': 'Koza, J.R. (1992). Genetic Programming: On the Programming of Computers by Means of Natural Selection.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'genetic-programming',\n",
       "  'original_term': 'Genetic Programming',\n",
       "  'linked_terms': []},\n",
       " 'randomized-algorithms': {'id': 38,\n",
       "  'term': 'Randomized Algorithms',\n",
       "  'definition': 'Algorithms that use random numbers to influence their behavior, often yielding faster or simpler solutions.',\n",
       "  'tags': ['probabilistic algorithms', 'complexity', 'optimization'],\n",
       "  'related_terms': ['Monte Carlo Methods', 'Las Vegas Algorithms'],\n",
       "  'examples': ['Using randomized quicksort to improve average-case performance.'],\n",
       "  'source': 'Motwani, R., & Raghavan, P. (1995). Randomized Algorithms.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'randomized-algorithms',\n",
       "  'original_term': 'Randomized Algorithms',\n",
       "  'linked_terms': []},\n",
       " 'computational-complexity': {'id': 39,\n",
       "  'term': 'Computational Complexity',\n",
       "  'definition': 'A field of study that analyzes the resources required for algorithms to solve problems, such as time and space.',\n",
       "  'tags': ['theoretical computer science', 'efficiency', 'algorithms'],\n",
       "  'related_terms': ['Big O Notation', 'P vs NP'],\n",
       "  'examples': ['Analyzing the time complexity of sorting algorithms.'],\n",
       "  'source': 'Sipser, M. (2012). Introduction to the Theory of Computation.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'computational-complexity',\n",
       "  'original_term': 'Computational Complexity',\n",
       "  'linked_terms': []},\n",
       " 'symbolic-ai': {'id': 40,\n",
       "  'term': 'Symbolic AI',\n",
       "  'definition': 'An approach to artificial intelligence that uses high-level, human-readable symbols and rules to represent knowledge and reasoning.',\n",
       "  'tags': ['knowledge_representation', 'logic', 'expert systems'],\n",
       "  'related_terms': ['Rule-Based Systems', 'Ontologies', 'Knowledge Graphs'],\n",
       "  'examples': ['Using a rule-based system to diagnose medical conditions.'],\n",
       "  'source': 'Russell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'symbolic-ai',\n",
       "  'original_term': 'Symbolic AI',\n",
       "  'linked_terms': []},\n",
       " 'skipgram': {'id': 42,\n",
       "  'term': 'Skipgram',\n",
       "  'definition': 'Skipgram is a Word2Vec architecture that learns word embeddings by predicting surrounding context words given a target word. It is effective for learning representations of rare words.',\n",
       "  'tags': ['natural_language_processing',\n",
       "   'word_embeddings',\n",
       "   'neural networks',\n",
       "   'representation_learning'],\n",
       "  'related_terms': ['Word2Vec',\n",
       "   'CBOW',\n",
       "   'Word Embedding',\n",
       "   'Distribution Hypothesis'],\n",
       "  'examples': [\"Predicting 'dog' and 'barks' given the target word 'loudly' in the sentence 'The dog barks loudly.'\"],\n",
       "  'source': 'Mikolov et al. (2013), arXiv:1301.3781; Wikipedia: https://en.wikipedia.org/wiki/Word2vec',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'skipgram',\n",
       "  'original_term': 'Skipgram',\n",
       "  'linked_terms': ['Word Embedding', 'Word2Vec']},\n",
       " 'word2vec': {'id': 43,\n",
       "  'term': 'Word2Vec',\n",
       "  'definition': 'Word2Vec is a neural embedding model that learns vector representations of words based on their context in large corpora. It uses architectures like CBOW and Skip-gram to predict words or contexts.',\n",
       "  'tags': ['natural_language_processing',\n",
       "   'word embedding',\n",
       "   'neural_networks',\n",
       "   'representation_learning'],\n",
       "  'related_terms': ['Skipgram',\n",
       "   'CBOW',\n",
       "   'Word Embedding',\n",
       "   'Distribution Hypothesis'],\n",
       "  'examples': [],\n",
       "  'source': 'Mikolov et al. (2013), arXiv:1301.3781; Wikipedia: https://en.wikipedia.org/wiki/Word2vec',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'word2vec',\n",
       "  'original_term': 'Word2Vec',\n",
       "  'linked_terms': []},\n",
       " 'distribution-hypothesis': {'id': 44,\n",
       "  'term': 'Distribution Hypothesis',\n",
       "  'definition': 'The distributional hypothesis states that words appearing in similar contexts tend to have similar meanings. It underpins many NLP techniques including word embeddings and semantic similarity models.',\n",
       "  'tags': ['natural_language_processing',\n",
       "   'semantics',\n",
       "   'distributional_semantics',\n",
       "   'representation_learning'],\n",
       "  'related_terms': ['Word Embedding',\n",
       "   'Word2Vec',\n",
       "   'Semantic Similarity',\n",
       "   'Contextual Embedding'],\n",
       "  'examples': [],\n",
       "  'source': 'Firth (1957); Wikipedia: https://en.wikipedia.org/wiki/Distributional_semantics',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'distribution-hypothesis',\n",
       "  'original_term': 'Distribution Hypothesis',\n",
       "  'linked_terms': ['Word Embedding']},\n",
       " 'text-representation': {'id': 45,\n",
       "  'term': 'Text Representation',\n",
       "  'definition': 'Text representation involves converting raw text into numerical formats suitable for machine learning. Techniques include bag-of-words, TF-IDF, and embeddings like Word2Vec and BERT.',\n",
       "  'tags': ['natural_language_processing',\n",
       "   'text_encoding',\n",
       "   'feature_extraction',\n",
       "   'representation_learning'],\n",
       "  'related_terms': ['Word Embedding',\n",
       "   'Word Representation',\n",
       "   'TF-IDF',\n",
       "   'Bag-of-Words'],\n",
       "  'examples': [],\n",
       "  'source': 'Jurafsky & Martin (2023), Speech and Language Processing; Wikipedia: https://en.wikipedia.org/wiki/Text_representation',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'text-representation',\n",
       "  'original_term': 'Text Representation',\n",
       "  'linked_terms': ['Word2Vec']},\n",
       " 'word-representation': {'id': 46,\n",
       "  'term': 'Word Representation',\n",
       "  'definition': 'Word representation refers to the way words are encoded for use in computational models. Traditional methods include one-hot encoding and TF-IDF, while modern approaches use dense embeddings that capture semantic relationships.',\n",
       "  'tags': ['natural_language_processing',\n",
       "   'representation_learning',\n",
       "   'word_embedding',\n",
       "   'semantic_similarity'],\n",
       "  'related_terms': ['Word Embedding',\n",
       "   'Word2Vec',\n",
       "   'Distributional Semantics',\n",
       "   'Text Representation'],\n",
       "  'examples': [],\n",
       "  'source': 'Jurafsky & Martin (2023), Speech and Language Processing; Wikipedia: https://en.wikipedia.org/wiki/Word_representation',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'word-representation',\n",
       "  'original_term': 'Word Representation',\n",
       "  'linked_terms': []},\n",
       " 'simulated-annealing': {'id': 47,\n",
       "  'term': 'Simulated Annealing',\n",
       "  'definition': \"A probabilistic optimization technique inspired by metallurgy's annealing process. It explores solution spaces by accepting worse solutions with a probability that decreases over time to avoid local optima.\",\n",
       "  'tags': ['optimization',\n",
       "   'metaheuristic',\n",
       "   'probabilistic_algorithm',\n",
       "   'search'],\n",
       "  'related_terms': ['Local Search',\n",
       "   'Metropolis Algorithm',\n",
       "   'Cooling Schedule',\n",
       "   'Hill Climbing'],\n",
       "  'examples': [],\n",
       "  'source': 'GeeksforGeeks, Cornell Open Textbook',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'simulated-annealing',\n",
       "  'original_term': 'Simulated Annealing',\n",
       "  'linked_terms': ['Neural Network']},\n",
       " 'monte-carlo-algorithm': {'id': 48,\n",
       "  'term': 'Monte Carlo Algorithm',\n",
       "  'definition': 'A randomized method that runs in fixed time and may yield incorrect results with some probability; error can be reduced via repetition.',\n",
       "  'tags': ['randomized_algorithm', 'probabilistic', 'algorithm_design'],\n",
       "  'related_terms': ['Las Vegas Algorithm', 'Randomized Algorithms'],\n",
       "  'examples': [],\n",
       "  'source': 'Duke University COMPSCI 330, CMU Harchol-Balter',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'monte-carlo-algorithm',\n",
       "  'original_term': 'Monte Carlo Algorithm',\n",
       "  'linked_terms': []},\n",
       " 'las-vegas-algorithm': {'id': 49,\n",
       "  'term': 'Las Vegas Algorithm',\n",
       "  'definition': 'A randomized algorithm that always returns the correct result (or explicitly indicates failure), but whose runtime is probabilistic. It must have finite expected runtime.',\n",
       "  'tags': ['randomized_algorithm', 'probabilistic', 'algorithm_design'],\n",
       "  'related_terms': ['Monte Carlo Algorithm', 'Randomized Algorithms'],\n",
       "  'examples': [],\n",
       "  'source': \"Wikipedia 'Las Vegas algorithm', Canny (1999 lecture)\",\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'las-vegas-algorithm',\n",
       "  'original_term': 'Las Vegas Algorithm',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'pseudocode': {'id': 50,\n",
       "  'term': 'Pseudocode',\n",
       "  'definition': 'A human-readable, high-level description of an algorithm combining natural language and programming-like structures. It is more precise than plain English but less formal than executable code.',\n",
       "  'tags': ['algorithm', 'documentation', 'algorithm_design', 'programming'],\n",
       "  'related_terms': ['Flowchart', 'Algorithm', 'Code', 'Pseudocode Syntax'],\n",
       "  'examples': [],\n",
       "  'source': \"de Weerdt (2019), Wikipedia 'Pseudocode'\",\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'pseudocode',\n",
       "  'original_term': 'Pseudocode',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'continual-learning': {'id': 51,\n",
       "  'term': 'Continual Learning',\n",
       "  'definition': 'Continual Learning (also known as lifelong learning) refers to the ability of a machine learning system to learn from a continuous stream of data over time, adapting to new tasks while retaining knowledge from previous ones. The key challenge in continual learning is avoiding catastrophic forgetting â€” the tendency of neural networks to forget previously learned information when trained on new data.',\n",
       "  'tags': ['machine_learning',\n",
       "   'lifelong_learning',\n",
       "   'online_learning',\n",
       "   'catastrophic_forgetting',\n",
       "   'adaptive_models'],\n",
       "  'related_terms': ['Catastrophic Forgetting',\n",
       "   'Elastic Weight Consolidation (EWC)',\n",
       "   'Replay Buffer',\n",
       "   'Online Learning',\n",
       "   'Transfer Learning',\n",
       "   'Meta-Learning'],\n",
       "  'examples': [],\n",
       "  'source': 'Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., & Wermter, S. (2019). *Continual lifelong learning with neural networks: A review*. Neural Networks.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'continual-learning',\n",
       "  'original_term': 'Continual Learning',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'random-forest': {'id': 52,\n",
       "  'term': 'Random Forest',\n",
       "  'definition': 'Random Forest is an ensemble learning algorithm that constructs a multitude of decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It combines the principles of bagging (bootstrap aggregating) and random feature selection to reduce variance and improve generalization. Random Forests are robust to overfitting and can handle high-dimensional data effectively.',\n",
       "  'tags': ['machine_learning,'],\n",
       "  'related_terms': ['Decision Tree',\n",
       "   'Bagging',\n",
       "   'Ensemble Learning',\n",
       "   'Feature Importance',\n",
       "   'Out-of-Bag Error'],\n",
       "  'examples': [],\n",
       "  'source': 'Breiman, L. (2001). *Random Forests*. Machine Learning.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'random-forest',\n",
       "  'original_term': 'Random Forest',\n",
       "  'linked_terms': ['Artificial Intelligence',\n",
       "   'Classification',\n",
       "   'Decision Tree',\n",
       "   'Deep Learning',\n",
       "   'Ensemble Learning',\n",
       "   'Generalization',\n",
       "   'Overfitting',\n",
       "   'Regression']},\n",
       " 'adaptive-random-forest-arf': {'id': 53,\n",
       "  'term': 'Adaptive Random Forest (ARF)',\n",
       "  'definition': 'Adaptive Random Forest (ARF) is an ensemble learning algorithm designed for evolving data streams. It extends the traditional Random Forest by incorporating mechanisms to detect and adapt to concept drift â€” changes in the underlying data distribution over time. ARF uses a collection of decision trees trained incrementally on streaming data, with each tree equipped with a drift detector and a background learner to replace outdated models when drift is detected.',\n",
       "  'tags': ['machine_learning',\n",
       "   'stream_learning',\n",
       "   'ensemble_learning',\n",
       "   'concept_drift',\n",
       "   'adaptive_models'],\n",
       "  'related_terms': ['Random Forest',\n",
       "   'Concept Drift',\n",
       "   'Online Learning',\n",
       "   'Drift Detection Method (DDM)',\n",
       "   'Hoeffding Tree',\n",
       "   'Streaming Data'],\n",
       "  'examples': [],\n",
       "  'source': 'Gomes, H. M., Bifet, A., Read, J., Barddal, J. P., Enembreck, F., Pfharinger, B., Holmes, G., & Abdessalem, T. (2017). *Adaptive Random Forests for Evolving Data Stream Classification*. Machine Learning Journal.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'adaptive-random-forest-arf',\n",
       "  'original_term': 'Adaptive Random Forest (ARF)',\n",
       "  'linked_terms': ['Artificial Intelligence',\n",
       "   'Concept Drift',\n",
       "   'Decision Tree',\n",
       "   'Ensemble Learning',\n",
       "   'Random Forest']},\n",
       " 'ensembles': {'id': 54,\n",
       "  'term': 'Ensembles',\n",
       "  'definition': 'In machine learning, an ensemble refers to a collection of models whose predictions are combined to produce a final output. The goal is to leverage the strengths of multiple models to improve accuracy, robustness, and generalization. Ensembles can be homogeneous (same model type) or heterogeneous (different model types), and they often outperform individual models, especially in competitions and real-world applications.',\n",
       "  'tags': ['machine_learning',\n",
       "   'ensemble_learning',\n",
       "   'model_combination',\n",
       "   'classification',\n",
       "   'regression'],\n",
       "  'related_terms': ['Bagging'],\n",
       "  'examples': [],\n",
       "  'source': 'Zhou, Z.-H. (2012). *Ensemble Methods: Foundations and Algorithms*.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'ensembles',\n",
       "  'original_term': 'Ensembles',\n",
       "  'linked_terms': ['Generalization']},\n",
       " 'naive-bayes': {'id': 55,\n",
       "  'term': 'Naive Bayes',\n",
       "  'definition': 'Naive Bayes is a family of probabilistic classifiers based on Bayesâ€™ Theorem, assuming strong (naive) independence between features. Despite this simplifying assumption, Naive Bayes classifiers often perform competitively in real-world tasks, especially in text classification and spam detection, due to their efficiency and robustness to irrelevant features.',\n",
       "  'tags': ['machine_learning,'],\n",
       "  'related_terms': ['Bayesâ€™ Theorem',\n",
       "   'Conditional Probability',\n",
       "   'Multinomial Naive Bayes',\n",
       "   'Gaussian Naive Bayes',\n",
       "   'Bernoulli Naive Bayes',\n",
       "   'Logistic Regression'],\n",
       "  'examples': [],\n",
       "  'source': 'Mitchell, T. M. (1997). *Machine Learning*. McGraw-Hill.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'naive-bayes',\n",
       "  'original_term': 'Naive Bayes',\n",
       "  'linked_terms': ['Artificial Intelligence', 'Classification']},\n",
       " 'decision-tree': {'id': 56,\n",
       "  'term': 'Decision Tree',\n",
       "  'definition': 'A decision tree is a supervised learning model used for classification and regression tasks. It represents decisions and their possible consequences as a tree-like structure, where internal nodes represent feature-based tests, branches represent outcomes of those tests, and leaf nodes represent predicted outputs. Decision trees are interpretable, non-parametric models that recursively partition the input space to minimize impurity or error.',\n",
       "  'tags': ['machine_learning',\n",
       "   'classification',\n",
       "   'regression',\n",
       "   'tree_model',\n",
       "   'interpretable_model'],\n",
       "  'related_terms': ['Random Forest',\n",
       "   'CART (Classification and Regression Trees)',\n",
       "   'Gini Impurity',\n",
       "   'Entropy',\n",
       "   'Pruning',\n",
       "   'Overfitting'],\n",
       "  'examples': [],\n",
       "  'source': 'Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). *Classification and Regression Trees*.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'decision-tree',\n",
       "  'original_term': 'Decision Tree',\n",
       "  'linked_terms': ['Classification', 'Regression', 'Supervised Learning']},\n",
       " 'k-nearest-neighbors-k-nn': {'id': 57,\n",
       "  'term': 'k-Nearest Neighbors (k-NN)',\n",
       "  'definition': 'k-Nearest Neighbors (kNN) is a non-parametric, instance-based learning algorithm used for classification and regression. It predicts the label of a new data point by identifying the kkk closest training examples in the feature space and using their labels to make a decision â€” typically via majority vote (classification) or averaging (regression). It assumes that similar instances exist in close proximity in the feature space.',\n",
       "  'tags': ['machine_learning',\n",
       "   'classification',\n",
       "   'regression',\n",
       "   'instance_based_learning',\n",
       "   'non_parametric'],\n",
       "  'related_terms': ['Distance Metrics',\n",
       "   'Lazy Learning',\n",
       "   'Decision Boundary',\n",
       "   'Curse of Dimensionality',\n",
       "   'Weighted kNN'],\n",
       "  'examples': [],\n",
       "  'source': 'Cover, T. M., & Hart, P. E. (1967). *Nearest neighbor pattern classification. IEEE Transactions on Information Theory*.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'k-nearest-neighbors-k-nn',\n",
       "  'original_term': 'k-Nearest Neighbors (k-NN)',\n",
       "  'linked_terms': ['Artificial Intelligence',\n",
       "   'Classification',\n",
       "   'Neural Network',\n",
       "   'Regression']},\n",
       " 'adaptive-machine-learning-adaptive-ml': {'id': 58,\n",
       "  'term': 'Adaptive Machine Learning (Adaptive ML)',\n",
       "  'definition': 'Adaptive Machine Learning refers to systems that dynamically learn and update their models in response to new or changing data without requiring explicit retraining on static datasets. These adaptive systems continuously ingest and process informationâ€”whether in real-time or batchesâ€”to adjust to evolving environments, detect and react to concept drift, and maintain accuracy over time.',\n",
       "  'tags': ['machine_learning',\n",
       "   'online_learning',\n",
       "   'concept_drift',\n",
       "   'real_time_learning',\n",
       "   'stream_learning',\n",
       "   'adaptive_systems'],\n",
       "  'related_terms': ['Online Learning',\n",
       "   'Incremental Learning',\n",
       "   'Concept Drift',\n",
       "   'Continual Learning',\n",
       "   'Streaming Data',\n",
       "   'Reinforcement Learning'],\n",
       "  'examples': [],\n",
       "  'source': 'Neha Singh & Khushi Chugh (2025), *â€œWhat is Adaptive Machine Learning and How Does It Work?â€* (Pickl.ai) [pickl.ai]',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'adaptive-machine-learning-adaptive-ml',\n",
       "  'original_term': 'Adaptive Machine Learning (Adaptive ML)',\n",
       "  'linked_terms': ['Artificial Intelligence', 'Concept Drift']},\n",
       " 'explainable-ai-xai': {'id': 59,\n",
       "  'term': 'Explainable AI (XAI)',\n",
       "  'definition': 'Explainable AI (XAI) comprises methods and processes designed to make the behavior, decisions, and internal reasoning of AI systems â€” especially complex \"black-box\" models â€” transparent, understandable, and auditable to human users. XAI seeks to build trust, enable accountability, and support oversight by revealing how inputs influence outputs based on interpretable artifacts like feature importances, saliency maps, or example-based explanations.',\n",
       "  'tags': ['machine_learning',\n",
       "   'interpretability',\n",
       "   'transparency',\n",
       "   'model_explainability',\n",
       "   'trustworthy_AI',\n",
       "   'auditing'],\n",
       "  'related_terms': ['Interpretability',\n",
       "   'Transparency',\n",
       "   'Post-hoc explanations',\n",
       "   'Ante-hoc (intrinsic) interpretable models',\n",
       "   'Saliency mapping',\n",
       "   'Counterfactual explanations'],\n",
       "  'examples': [],\n",
       "  'source': 'Wikipedia: *â€œExplainable artificial intelligenceâ€* [en.wikipedia.org]',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'explainable-ai-xai',\n",
       "  'original_term': 'Explainable AI (XAI)',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'bootstrap-aggregating-bagging': {'id': 60,\n",
       "  'term': 'Bootstrap Aggregating (Bagging)',\n",
       "  'definition': 'Bootstrap Aggregating, or Bagging, is an ensemble learning technique that improves the stability and accuracy of machine learning algorithms by training multiple models on different bootstrapped subsets of the training data and aggregating their predictions. It reduces variance and helps prevent overfitting, especially in high-variance models like decision trees.',\n",
       "  'tags': ['ensemble_learning',\n",
       "   'bagging',\n",
       "   'bootstrap',\n",
       "   'variance_reduction',\n",
       "   'machine_learning'],\n",
       "  'related_terms': ['Random Forest',\n",
       "   'Bootstrap Sampling',\n",
       "   'Ensemble Learning',\n",
       "   'Variance',\n",
       "   'Decision Tree'],\n",
       "  'examples': [],\n",
       "  'source': 'Breiman, L. (1996). *Bagging Predictors*. Machine Learning.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'bootstrap-aggregating-bagging',\n",
       "  'original_term': 'Bootstrap Aggregating (Bagging)',\n",
       "  'linked_terms': ['Artificial Intelligence',\n",
       "   'Decision Tree',\n",
       "   'Ensemble Learning',\n",
       "   'Overfitting']},\n",
       " 'ensemble-learning': {'id': 61,\n",
       "  'term': 'Ensemble Learning',\n",
       "  'definition': 'Ensemble learning is a machine learning paradigm where multiple models (often called \"base learners\" or \"weak learners\") are trained and combined to solve the same problem. The goal is to improve predictive performance, robustness, and generalization by aggregating the outputs of diverse models. Ensembles can reduce variance (e.g., bagging), bias (e.g., boosting), or improve predictions through voting or averaging.',\n",
       "  'tags': ['machine_learning',\n",
       "   'ensemble_methods',\n",
       "   'model_combination',\n",
       "   'bagging',\n",
       "   'boosting'],\n",
       "  'related_terms': ['Bagging',\n",
       "   'Boosting',\n",
       "   'Stacking',\n",
       "   'Random Forest',\n",
       "   'Voting Classifier',\n",
       "   'Bootstrap Aggregating'],\n",
       "  'examples': [],\n",
       "  'source': 'Zhou, Z.-H. (2012). *Ensemble Methods: Foundations and Algorithms*.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'ensemble-learning',\n",
       "  'original_term': 'Ensemble Learning',\n",
       "  'linked_terms': ['Artificial Intelligence', 'Ensembles', 'Generalization']},\n",
       " 'tournament-selection': {'id': 62,\n",
       "  'term': 'Tournament Selection',\n",
       "  'definition': 'Tournament selection is a method used in evolutionary algorithms to select individuals for reproduction. A subset of individuals is randomly chosen from the population, and the one with the highest fitness among them is selected. This process is repeated to fill the mating pool. Tournament size controls selection pressure: larger tournaments increase the chance of selecting fitter individuals.',\n",
       "  'tags': ['evolutionary_algorithm',\n",
       "   'selection_method',\n",
       "   'genetic_algorithm',\n",
       "   'genetic_programming',\n",
       "   'fitness_based_selection'],\n",
       "  'related_terms': ['Fitness Function',\n",
       "   'Lexicase Selection',\n",
       "   'Roulette Wheel Selection',\n",
       "   'Rank Selection',\n",
       "   'Elitism'],\n",
       "  'examples': [],\n",
       "  'source': 'Goldberg, D. E., & Deb, K. (1991). *A comparative analysis of selection schemes used in genetic algorithms*.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'tournament-selection',\n",
       "  'original_term': 'Tournament Selection',\n",
       "  'linked_terms': ['Machine Learning']},\n",
       " 'parsimony-pressure': {'id': 63,\n",
       "  'term': 'Parsimony Pressure',\n",
       "  'definition': 'Parsimony pressure is a technique used in genetic programming and evolutionary algorithms to control bloat by penalizing overly complex solutions. It modifies the fitness function or selection criteria to favor simpler individuals, encouraging the evolution of more compact and generalizable models without sacrificing performance.',\n",
       "  'tags': ['genetic_programming',\n",
       "   'evolutionary_algorithm',\n",
       "   'bloat_control',\n",
       "   'model_complexity',\n",
       "   'regularization'],\n",
       "  'related_terms': ['Bloat',\n",
       "   'Fitness Function',\n",
       "   'Regularization',\n",
       "   'Minimum Description Length (MDL)',\n",
       "   'Introns'],\n",
       "  'examples': [],\n",
       "  'source': 'Poli, R., Langdon, W. B., & McPhee, N. F. (2008). *A Field Guide to Genetic Programming*',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'parsimony-pressure',\n",
       "  'original_term': 'Parsimony Pressure',\n",
       "  'linked_terms': ['Bloat', 'Fitness Function', 'Genetic Programming']},\n",
       " 'bloat': {'id': 64,\n",
       "  'term': 'Bloat',\n",
       "  'definition': 'Bloat refers to the uncontrolled growth of solution representations (e.g., expression trees in genetic programming) without corresponding improvements in fitness. It often results in overly complex, inefficient, and less generalizable models. Bloat is a common issue in genetic programming due to the tendency of evolutionary operators like crossover and mutation to increase program size over generations.',\n",
       "  'tags': ['genetic_programming',\n",
       "   'evolutionary_algorithm',\n",
       "   'model_complexity',\n",
       "   'overfitting',\n",
       "   'optimization'],\n",
       "  'related_terms': ['Parsimony Pressure',\n",
       "   'Overfitting',\n",
       "   'Genetic Drift',\n",
       "   'Introns',\n",
       "   'Code Growth'],\n",
       "  'examples': [],\n",
       "  'source': 'Poli, R., Langdon, W. B., & McPhee, N. F. (2008). *A Field Guide to Genetic Programming*.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'bloat',\n",
       "  'original_term': 'Bloat',\n",
       "  'linked_terms': ['Crossover',\n",
       "   'Expression Tree',\n",
       "   'Genetic Programming',\n",
       "   'Mutation']},\n",
       " 'underfitting': {'id': 65,\n",
       "  'term': 'Underfitting',\n",
       "  'definition': 'Underfitting occurs when a machine learning model is too simple to capture the underlying structure of the data. It results in poor performance on both the training and test sets. Underfitting is typically caused by high bias, insufficient model complexity, or inadequate training time.',\n",
       "  'tags': ['machine_learning',\n",
       "   'model_evaluation',\n",
       "   'bias',\n",
       "   'generalization',\n",
       "   'training_error'],\n",
       "  'related_terms': ['Overfitting',\n",
       "   'Generalization',\n",
       "   'Bias-Variance Trade-off',\n",
       "   'Model Complexity',\n",
       "   'Regularization'],\n",
       "  'examples': [],\n",
       "  'source': 'Goodfellow, Bengio & Courville (2016), *Deep Learning*, MIT Press',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'underfitting',\n",
       "  'original_term': 'Underfitting',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'overfitting': {'id': 66,\n",
       "  'term': 'Overfitting',\n",
       "  'definition': 'Overfitting occurs when a machine learning model learns not only the underlying patterns in the training data but also the noise and random fluctuations. As a result, the model performs well on training data but poorly on unseen data. Overfitting is a key obstacle to generalization and is more likely when models are too complex relative to the amount of training data.',\n",
       "  'tags': ['machine_learning',\n",
       "   'model_evaluation',\n",
       "   'generalization',\n",
       "   'bias_variance',\n",
       "   'training_error'],\n",
       "  'related_terms': ['Underfitting',\n",
       "   'Generalization',\n",
       "   'Bias-Variance Trade-off',\n",
       "   'Regularization',\n",
       "   'Cross-Validation'],\n",
       "  'examples': [],\n",
       "  'source': 'Goodfellow, Bengio & Courville (2016), *Deep Learning*, MIT Press',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'overfitting',\n",
       "  'original_term': 'Overfitting',\n",
       "  'linked_terms': ['Artificial Intelligence', 'Generalization']},\n",
       " 'bias-variance-trade-off': {'id': 67,\n",
       "  'term': 'Bias-Variance Trade-off',\n",
       "  'definition': 'The bias-variance trade-off is a fundamental concept in statistical learning theory that describes the tension between two sources of error in predictive models:',\n",
       "  'tags': ['machine_learning model_evaluation',\n",
       "   'generalization',\n",
       "   'overfitting',\n",
       "   'underfitting'],\n",
       "  'related_terms': ['Overfitting',\n",
       "   'Underfitting',\n",
       "   'Generalization',\n",
       "   'Regularization',\n",
       "   'Cross-Validation'],\n",
       "  'examples': [],\n",
       "  'source': 'Geman, S., Bienenstock, E., & Doursat, R. (1992). *Neural networks and the bias/variance dilemma*. Neural Computation.',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'bias-variance-trade-off',\n",
       "  'original_term': 'Bias-Variance Trade-off',\n",
       "  'linked_terms': []},\n",
       " 'generalization': {'id': 68,\n",
       "  'term': 'Generalization',\n",
       "  'definition': 'In machine learning, generalization refers to a modelâ€™s ability to perform well on unseen data drawn from the same distribution as the training set. A well-generalized model captures the underlying patterns in the data without overfitting to noise or specific examples. Generalization is the ultimate goal of supervised learning and is typically evaluated using validation or test sets.',\n",
       "  'tags': ['machine_learning',\n",
       "   'model_evaluation',\n",
       "   'overfitting',\n",
       "   'bias_variance',\n",
       "   'generalization_error'],\n",
       "  'related_terms': ['Overfitting',\n",
       "   'Underfitting',\n",
       "   'Bias-Variance Trade-off',\n",
       "   'Cross-Validation',\n",
       "   'Test Error'],\n",
       "  'examples': [],\n",
       "  'source': 'Goodfellow, Bengio & Courville (2016), *Deep Learning*, MIT Press',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'generalization',\n",
       "  'original_term': 'Generalization',\n",
       "  'linked_terms': ['Artificial Intelligence',\n",
       "   'Overfitting',\n",
       "   'Supervised Learning']},\n",
       " 'mutation': {'id': 69,\n",
       "  'term': 'Mutation',\n",
       "  'definition': 'Mutation is a genetic operator used in evolutionary algorithms to introduce random variation into individuals. It modifies parts of a solution (e.g., a gene, subtree, or node) to maintain genetic diversity and explore new areas of the search space. Mutation helps prevent premature convergence and supports exploration.',\n",
       "  'tags': ['evolutionary_algorithm',\n",
       "   'genetic_programming',\n",
       "   'genetic_algorithm',\n",
       "   'variation_operator',\n",
       "   'diversity'],\n",
       "  'related_terms': ['Crossover',\n",
       "   'Selection',\n",
       "   'Genetic Drift',\n",
       "   'Hill Climbing',\n",
       "   'Exploration vs. Exploitation'],\n",
       "  'examples': [],\n",
       "  'source': '',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'mutation',\n",
       "  'original_term': 'Mutation',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'crossover': {'id': 70,\n",
       "  'term': 'Crossover',\n",
       "  'definition': 'Crossover is a genetic operator used in evolutionary algorithms to combine the genetic information of two parent solutions to generate new offspring. It mimics biological reproduction by exchanging substructures (e.g., genes, subtrees, or substrings) between individuals, promoting exploration of the solution space.',\n",
       "  'tags': ['evolutionary_algorithm',\n",
       "   'genetic_programming',\n",
       "   'genetic_algorithm',\n",
       "   'recombination',\n",
       "   'variation_operator'],\n",
       "  'related_terms': ['Mutation Selection',\n",
       "   'Genetic Programming',\n",
       "   'Subtree Crossover',\n",
       "   'Uniform Crossover',\n",
       "   'One-point Crossover'],\n",
       "  'examples': [],\n",
       "  'source': 'Koza, J. R. (1992). *Genetic Programming: On the Programming of Computers by Means of Natural Selection*',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'crossover',\n",
       "  'original_term': 'Crossover',\n",
       "  'linked_terms': []},\n",
       " 'lexicase-selection': {'id': 71,\n",
       "  'term': 'Lexicase Selection',\n",
       "  'definition': 'A parent (and sometimes survivor) selection method in evolutionary computation that processes training cases one at a time in random order. Instead of aggregating performance across all cases, it filters the population stepwise: individuals that perform best on the current case advance to be evaluated on the next case, and so on until only one remains or test cases are exhausted.',\n",
       "  'tags': ['evolutionary_algorithm',\n",
       "   'genetic_programming',\n",
       "   'parent_selection',\n",
       "   'non_aggregate_selection',\n",
       "   'diversity_maintenance'],\n",
       "  'related_terms': ['Tournament Selection',\n",
       "   'Fitness-Proportionate Selection',\n",
       "   'Îµ-Lexicase Selection',\n",
       "   'Down-Sampled Lexicase Selection',\n",
       "   'Pareto Selection'],\n",
       "  'examples': [],\n",
       "  'source': '',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'lexicase-selection',\n",
       "  'original_term': 'Lexicase Selection',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'fitness-function': {'id': 72,\n",
       "  'term': 'Fitness Function',\n",
       "  'definition': 'A fitness function is a specific kind of objective (or cost) function used in evolutionary algorithms (EAs). It assigns a numerical score to each candidate solution, indicating how well it satisfies the optimization goals. It guides selection, reproduction, and progression of the population toward better solutions.',\n",
       "  'tags': ['evolutionary_algorithm',\n",
       "   'optimization',\n",
       "   'objective_function',\n",
       "   'genetic_programming',\n",
       "   'genetic_algorithm'],\n",
       "  'related_terms': ['Genetic Algorithm (GA)',\n",
       "   'Genetic Programming (GP)',\n",
       "   'Objective Function',\n",
       "   'Cost Function',\n",
       "   'Selection Methods (e.g.',\n",
       "   'Tournament Selection',\n",
       "   'Roulette Wheel)'],\n",
       "  'examples': [],\n",
       "  'source': 'Wikipedia: \"Fitness function\" [en.wikipedia.org]',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'fitness-function',\n",
       "  'original_term': 'Fitness Function',\n",
       "  'linked_terms': []},\n",
       " 'expression-tree': {'id': 73,\n",
       "  'term': 'Expression Tree',\n",
       "  'definition': 'A tree data structure representing mathematical expressions, where leaf nodes are operands (variables/constants) and internal nodes represent operators or functions. Widely used in symbolic regression and compiler design to manipulate, evaluate, or optimize expressions.',\n",
       "  'tags': ['data_structure',\n",
       "   'symbolic_regression',\n",
       "   'compilers',\n",
       "   'AST',\n",
       "   'expression_parsing'],\n",
       "  'related_terms': ['Abstract Syntax Tree',\n",
       "   'Symbolic Regression',\n",
       "   'Crossover',\n",
       "   'Mutation'],\n",
       "  'examples': ['Representation of $sin(x) + 0$.$5*log(x + 2)$: a tree with root $+$',\n",
       "   'left child $sin(x)$',\n",
       "   'right subtree $*$ with children $0.5$ and $log(x+2)$. [ml4science.com]'],\n",
       "  'source': 'Wikipedia â€œSymbolic regressionâ€ (description of expression tree usage) [en.wikipedia.org]',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'expression-tree',\n",
       "  'original_term': 'Expression Tree',\n",
       "  'linked_terms': ['Regression', 'Symbolic Regression']},\n",
       " 'monte-carlo-tree-search-mcts': {'id': 74,\n",
       "  'term': 'Monte Carlo Tree Search (MCTS)',\n",
       "  'definition': 'A heuristic tree-based search algorithm employing Monte Carlo simulations and selective node expansion to tackle vast decision spaces. It alternates between selection, expansion, simulation, and backpropagation guided by the Upper Confidence Bounds for Trees (UCT) criterion.',\n",
       "  'tags': ['search_algorithm',\n",
       "   'heuristic_search',\n",
       "   'game_ai',\n",
       "   'probabilistic',\n",
       "   'UCT',\n",
       "   'tree_search'],\n",
       "  'related_terms': ['Heuristic Search',\n",
       "   'UCT',\n",
       "   'Game Theory',\n",
       "   'Reinforcement Learning'],\n",
       "  'examples': [],\n",
       "  'source': 'Wikipedia â€œMonte Carlo tree searchâ€ [en.wikipedia.org]',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'monte-carlo-tree-search-mcts',\n",
       "  'original_term': 'Monte Carlo Tree Search (MCTS)',\n",
       "  'linked_terms': ['Backpropagation']},\n",
       " 'bayesian-symbolic-regression': {'id': 75,\n",
       "  'term': 'Bayesian Symbolic Regression',\n",
       "  'definition': 'A probabilistic symbolic regression method that infers closed-form expressions by sampling from a posterior distribution over expression trees. It integrates prior beliefs, quantifies uncertainty, and promotes simplicity through a Bayesian framework, often implemented with MCMC or Sequential Monte Carlo.',\n",
       "  'tags': ['symbolic_regression',\n",
       "   'Bayesian_methods',\n",
       "   'probabilistic_modeling',\n",
       "   'uncertainty_quantification',\n",
       "   'interpretability'],\n",
       "  'related_terms': ['Symbolic Regression',\n",
       "   'Expression Trees',\n",
       "   'Posterior Sampling',\n",
       "   'MCMC',\n",
       "   'SMC'],\n",
       "  'examples': [],\n",
       "  'source': 'Ying\\u202fJin et\\u202fal., Bayesian Symbolic Regression (arXiv:1910.08892) [arxiv.org]',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'bayesian-symbolic-regression',\n",
       "  'original_term': 'Bayesian Symbolic Regression',\n",
       "  'linked_terms': ['Artificial Intelligence',\n",
       "   'Expression Tree',\n",
       "   'Regression',\n",
       "   'Symbolic Regression']},\n",
       " 'serviceoriented-computing-soc': {'id': 76,\n",
       "  'term': 'Serviceâ€‘oriented Computing (SOC)',\n",
       "  'definition': 'A design paradigm where discrete, networkâ€‘accessible software services expose functionality via wellâ€‘defined interfaces, enabling loose coupling, interoperability, reusability, and service orchestration.',\n",
       "  'tags': ['software_architecture', 'services', 'SOA', 'integration'],\n",
       "  'related_terms': ['Microservices',\n",
       "   'API',\n",
       "   'Enterprise Service Bus (ESB)',\n",
       "   'Service Registry',\n",
       "   'Web Services'],\n",
       "  'examples': [],\n",
       "  'source': 'â€œServiceâ€‘oriented architectureâ€, IBM, GeeksforGeeks',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'serviceoriented-computing-soc',\n",
       "  'original_term': 'Serviceâ€‘oriented Computing (SOC)',\n",
       "  'linked_terms': []},\n",
       " 'implicit-graphs': {'id': 77,\n",
       "  'term': 'Implicit Graphs',\n",
       "  'definition': 'Graph representations where vertices and edges are not explicitly stored but generated dynamically via computational rules or functions. Common in large state spaces like puzzles or game trees.',\n",
       "  'tags': ['graph', 'implicit_representation', 'search', 'state_space'],\n",
       "  'related_terms': ['Graph Search', 'BFS', 'DFS', 'Stateâ€Space Graphs'],\n",
       "  'examples': [],\n",
       "  'source': 'Wikipedia â€œImplicit graphâ€, expert guide',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'implicit-graphs',\n",
       "  'original_term': 'Implicit Graphs',\n",
       "  'linked_terms': []},\n",
       " 'depth-first-search-dfs': {'id': 78,\n",
       "  'term': 'Depth First Search (DFS)',\n",
       "  'definition': 'A graph/traversal search that explores as far down each branch as possible before backtracking, typically implemented recursively or with a stack. It visits each reachable node once using backtracking order.',\n",
       "  'tags': ['graph_search', 'depth_first', 'traversal', 'stack', 'recursive'],\n",
       "  'related_terms': ['BFS',\n",
       "   'Graph Search',\n",
       "   'Connected Components',\n",
       "   'Backtracking'],\n",
       "  'examples': ['On a branching graph from node\\u202f0: visits 0â†’1â†’2â†’3â†’4, returning and continuing depthâ€wise at each node.'],\n",
       "  'source': 'Wikipedia â€œDepth-first searchâ€, GeeksforGeeks [en.wikipedia.org], [geeksforgeeks.org] [geeksforgeeks.org], [en.wikipedia.org]',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'depth-first-search-dfs',\n",
       "  'original_term': 'Depth First Search (DFS)',\n",
       "  'linked_terms': []},\n",
       " 'breadth-first-search-bfs': {'id': 79,\n",
       "  'term': 'Breadth First Search (BFS)',\n",
       "  'definition': 'A graphâ€traversal algorithm that explores vertices level by level, using a FIFO queue. It marks visited nodes to prevent repetition and can be used to find shortest paths in unweighted graphs.',\n",
       "  'tags': ['graph_search', 'breadth_first', 'traversal', 'shortest_path'],\n",
       "  'related_terms': ['Depth First Search',\n",
       "   'Graph Search',\n",
       "   'Queue',\n",
       "   'Connected Components'],\n",
       "  'examples': [],\n",
       "  'source': 'Wikipedia â€œBreadth-first searchâ€, GeeksforGeeks  [geeksforgeeks.org], [en.wikipedia.org] [en.wikipedia.org], [geeksforgeeks.org]',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'breadth-first-search-bfs',\n",
       "  'original_term': 'Breadth First Search (BFS)',\n",
       "  'linked_terms': []},\n",
       " 'graph-connectivity': {'id': 80,\n",
       "  'term': 'Graph Connectivity',\n",
       "  'definition': \"In graph theory, connectivity measures whether vertices are reachable from each other. In undirected graphs, connectedness means there's a path between every vertex pair. Vertexâ€connectivity (Îº) is the minimum number of vertices whose removal disconnects the graph; edgeâ€connectivity (Î») is similarly defined for edges.\",\n",
       "  'tags': ['graph_theory', 'connectivity', 'resilience', 'network'],\n",
       "  'related_terms': ['Connected Component',\n",
       "   'Vertex Cut',\n",
       "   'Edge Cut',\n",
       "   'kâ€‘connected Graph'],\n",
       "  'examples': ['A tree graph is 1â€‘edgeâ€‘connected: removing any edge disconnects it. A complete graph Kâ‚™ has Îº = Î» = n\\u202fâˆ’\\u202f1.'],\n",
       "  'source': 'Wikipedia â€œConnectivity (graph theory)â€, Algocademy [en.wikipedia.org], [algocademy.com]',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'graph-connectivity',\n",
       "  'original_term': 'Graph Connectivity',\n",
       "  'linked_terms': ['Artificial Intelligence', 'Neural Network']},\n",
       " 'greedy-search-greedy-algorithm': {'id': 81,\n",
       "  'term': 'Greedy Search (Greedy Algorithm)',\n",
       "  'definition': 'A heuristic or algorithmic approach making the locally optimal choice at each step with the intent of finding a global optimum. It doesnâ€™t backtrack or reconsider earlier selections; correctness depends on the problem having the greedyâ€choice property.',\n",
       "  'tags': ['algorithm', 'heuristic', 'optimization', 'greedy'],\n",
       "  'related_terms': ['Dynamic Programming',\n",
       "   'Local Search',\n",
       "   'Fractional Knapsack',\n",
       "   'Dijkstraâ€™s Algorithm'],\n",
       "  'examples': ['Coin-change in canonical systemsâ€”always pick largest denomination less than remaining amount.'],\n",
       "  'source': 'Wikipedia â€œGreedy algorithmâ€, GeeksforGeeks',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'greedy-search-greedy-algorithm',\n",
       "  'original_term': 'Greedy Search (Greedy Algorithm)',\n",
       "  'linked_terms': []},\n",
       " 'traveling-salesman-problem-tsp': {'id': 82,\n",
       "  'term': 'Traveling Salesman Problem (TSP)',\n",
       "  'definition': 'Combinatorial optimizationâ€”in an undirected weighted graph with n vertices, find the minimumâ€weight Hamiltonian cycle visiting each vertex exactly once and returning to start.',\n",
       "  'tags': ['NP-hard',\n",
       "   'combinatorial_optimization',\n",
       "   'graph',\n",
       "   'benchmark_problem'],\n",
       "  'related_terms': ['Hamiltonian Cycle',\n",
       "   'NP-hard Problems',\n",
       "   'Approximation Algorithms',\n",
       "   'Heuristics',\n",
       "   'Knapsack Problem'],\n",
       "  'examples': ['TSP is used in planning microchip manufacturing drills, DNA assembly, logistics routing.'],\n",
       "  'source': 'Wikipedia, Encyclopaedia Britannica',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'traveling-salesman-problem-tsp',\n",
       "  'original_term': 'Traveling Salesman Problem (TSP)',\n",
       "  'linked_terms': []},\n",
       " 'graph-search': {'id': 83,\n",
       "  'term': 'Graph Search',\n",
       "  'definition': 'A family of algorithms for systematically visiting the vertices and edges of a graph to compute properties such as reachability, shortest paths, or connectivity. Implementations typically track visited nodes to avoid repetition and may be refined into specialized strategies like BFS or DFS.',\n",
       "  'tags': ['graph', 'search', 'traversal', 'algorithm'],\n",
       "  'related_terms': ['Breadth First Search',\n",
       "   'Depth First Search',\n",
       "   'Shortest Path',\n",
       "   'Connectivity'],\n",
       "  'examples': ['A general vertexâ€hopping routine iterates through each unvisited vertex, visits it, and recurses or enqueues neighbors.'],\n",
       "  'source': 'Wikipedia â€œGraph traversalâ€, CMU CS lecture notes',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'graph-search',\n",
       "  'original_term': 'Graph Search',\n",
       "  'linked_terms': ['Neural Network']},\n",
       " 'tabu-search': {'id': 84,\n",
       "  'term': 'Tabu Search',\n",
       "  'definition': 'A metaheuristic augmenting local search with adaptive memory (tabu list) to avoid revisiting solutions and to escape local optima. It leverages short-term memory and aspiration criteria for strategic exploration.',\n",
       "  'tags': ['metaheuristic',\n",
       "   'local_search',\n",
       "   'optimization',\n",
       "   'combinatorial_optimization'],\n",
       "  'related_terms': ['Local Search',\n",
       "   'Simulated Annealing',\n",
       "   'Genetic Algorithms'],\n",
       "  'examples': ['In a scheduling problem, recently swapped tasks are marked â€œtabuâ€ for a fixed tenure to prevent undoing unless aspiration criteria are met.'],\n",
       "  'source': 'Glover et\\u202fal. (1997), Wikipedia â€œTabu Searchâ€',\n",
       "  'last_updated': '2025-10-23',\n",
       "  'key_slug': 'tabu-search',\n",
       "  'original_term': 'Tabu Search',\n",
       "  'linked_terms': []},\n",
       " 'simulated-annealing-2': {'id': 85,\n",
       "  'term': 'Simulated Annealing',\n",
       "  'definition': \"A probabilistic optimization technique inspired by metallurgy's annealing process. It explores solution spaces by accepting worse solutions with a probability that decreases over time to avoid local optima.\",\n",
       "  'tags': ['optimization',\n",
       "   'metaheuristic',\n",
       "   'probabilistic_algorithm',\n",
       "   'search'],\n",
       "  'related_terms': ['Local Search',\n",
       "   'Metropolis Algorithm',\n",
       "   'Cooling Schedule',\n",
       "   'Hill Climbing'],\n",
       "  'examples': ['Temperature T initialized high, neighbor solution Sâ€² accepted if cost lower or with probability exp(â€“Î”E/T).'],\n",
       "  'source': 'GeeksforGeeks, Cornell Open Textbook',\n",
       "  'last_updated': '2025-10-25',\n",
       "  'key_slug': 'simulated-annealing-2',\n",
       "  'original_term': 'Simulated Annealing',\n",
       "  'linked_terms': ['Neural Network']},\n",
       " 'word-embedding': {'id': 86,\n",
       "  'term': 'Word Embedding',\n",
       "  'definition': 'A dense, continuous vector representation of words that encodes semantic and syntactic information by placing words with similar meanings close together in a lower-dimensional space, learned from large corpora based on distributional patterns.[ibm.com], [scirp.org]',\n",
       "  'tags': ['Representation Learning',\n",
       "   'Vector Semantics',\n",
       "   'NLP',\n",
       "   'Distributional Hypothesis'],\n",
       "  'related_terms': ['Word2Vec',\n",
       "   'GloVe',\n",
       "   'Contextual Embeddings',\n",
       "   'Distributional Hypothesis'],\n",
       "  'examples': [],\n",
       "  'source': 'Worth, P. J. (2023). Word Embeddings and Semantic Spaces in Natural Language Processing. International Journal of Intelligence Science, 13(1), 1â€“21. [scirp.org]',\n",
       "  'last_updated': '2025-10-29',\n",
       "  'key_slug': 'word-embedding',\n",
       "  'original_term': 'Word Embedding',\n",
       "  'linked_terms': []},\n",
       " 'softmax-function': {'id': 87,\n",
       "  'term': 'Softmax Function',\n",
       "  'definition': 'A normalized exponential function that converts a vector of real-valued scores (logits) into a probability distribution over multiple classes, with output values between 0 and 1 that sum to 1. [en.wikipedia.org], [geeksforgeeks.org]',\n",
       "  'tags': ['Activation Function',\n",
       "   'Classification',\n",
       "   'Probability',\n",
       "   'Neural Networks'],\n",
       "  'related_terms': ['Cross-Entropy',\n",
       "   'Logits',\n",
       "   'Multinomial Logistic Regression',\n",
       "   'Sigmoid'],\n",
       "  'examples': [],\n",
       "  'source': '*â€œSoftmax functionâ€*, Wikipedia, retrieved 2025; also described in GeeksforGeeks (2025). [en.wikipedia.org] [geeksforgeeks.org]',\n",
       "  'last_updated': '2025-10-29',\n",
       "  'key_slug': 'softmax-function',\n",
       "  'original_term': 'Softmax Function',\n",
       "  'linked_terms': []},\n",
       " 'causal-mask': {'id': 88,\n",
       "  'term': 'Causal Mask',\n",
       "  'definition': 'A binary mask applied in attention mechanisms to prevent a token from attending to future positions in a sequence; ensures autoregressive (left-to-right) modeling, maintaining causality. [codegenes.net], [geeksforgeeks.org]',\n",
       "  'tags': ['Attention Mechanism',\n",
       "   'Transformer',\n",
       "   'Autoregressive',\n",
       "   'Language Modeling'],\n",
       "  'related_terms': ['Self-Attention',\n",
       "   'Transformer',\n",
       "   'Masked Language Modeling',\n",
       "   'GPT'],\n",
       "  'examples': [],\n",
       "  'source': 'Codegenes blog tutorial (2025) and GeeksforGeeks (2025) on causal language models. [codegenes.net], [geeksforgeeks.org]',\n",
       "  'last_updated': '2025-10-29',\n",
       "  'key_slug': 'causal-mask',\n",
       "  'original_term': 'Causal Mask',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'sequence-to-sequence-model': {'id': 89,\n",
       "  'term': 'Sequence-to-Sequence Model',\n",
       "  'definition': 'A neural architecture comprising an encoder and a decoderâ€”often RNNs, LSTMs, or Transformersâ€”that maps variable-length input sequences to output sequences, compressed via a context vector (or using attention). [geeksforgeeks.org], [ultralytics.com]',\n",
       "  'tags': ['Encoderâ€“Decoder',\n",
       "   'Translation',\n",
       "   'Summarization',\n",
       "   'Speech-to-Text',\n",
       "   'NLP'],\n",
       "  'related_terms': ['Encoder',\n",
       "   'Decoder',\n",
       "   'Attention Mechanism',\n",
       "   'Neural Machine Translation',\n",
       "   'Context Vector'],\n",
       "  'examples': [],\n",
       "  'source': 'GeeksforGeeks entry (Oct 2025); Ultralytics glossary (2025). [geeksforgeeks.org] [ultralytics.com]',\n",
       "  'last_updated': '2025-10-29',\n",
       "  'key_slug': 'sequence-to-sequence-model',\n",
       "  'original_term': 'Sequence-to-Sequence Model',\n",
       "  'linked_terms': ['Context Vector', 'Neural Network', 'Transformer']},\n",
       " 'recurrent-neural-network-rnn': {'id': 90,\n",
       "  'term': 'Recurrent Neural Network (RNN)',\n",
       "  'definition': \"A neural network architecture for sequential data where each time-step's output is fed back as input to the next, maintaining a hidden state (â€œmemoryâ€) to model temporal dependencies. [en.wikipedia.org], [geeksforgeeks.org]\",\n",
       "  'tags': ['Recurrent Architectures',\n",
       "   'Sequence Modeling',\n",
       "   'Time Series',\n",
       "   'Speech',\n",
       "   'Text'],\n",
       "  'related_terms': ['LSTM',\n",
       "   'GRU',\n",
       "   'Backpropagation Through Time',\n",
       "   'Encoderâ€“Decoder',\n",
       "   'Sequence-to-Sequence'],\n",
       "  'examples': [],\n",
       "  'source': '*â€œRecurrent neural networkâ€*, Wikipedia (2025); GeeksforGeeks introduction to RNNs (2025). [en.wikipedia.org] [geeksforgeeks.org]',\n",
       "  'last_updated': '2025-10-29',\n",
       "  'key_slug': 'recurrent-neural-network-rnn',\n",
       "  'original_term': 'Recurrent Neural Network (RNN)',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'bleu-bilingual-evaluation-understudy': {'id': 91,\n",
       "  'term': 'BLEU (Bilingual Evaluation Understudy)',\n",
       "  'definition': 'An automatic metric for evaluating translation quality, measuring nâ€‘gram precision of machineâ€‘generated text against one or more reference translations, applying a brevity penalty to mitigate overly short outputs. [geeksforgeeks.org], [en.wikipedia.org]',\n",
       "  'tags': ['Evaluation Metric', 'Machine Translation', 'NLP'],\n",
       "  'related_terms': ['ROUGE', 'METEOR', 'SacreBLEU', 'Nâ€‘gram Overlap'],\n",
       "  'examples': [],\n",
       "  'source': 'Papineni et al. (2002), â€œBLEU: a Method for Automatic Evaluation of Machine Translationâ€; overview on Wikipedia. [en.wikipedia.org]',\n",
       "  'last_updated': '2025-11-02',\n",
       "  'key_slug': 'bleu-bilingual-evaluation-understudy',\n",
       "  'original_term': 'BLEU (Bilingual Evaluation Understudy)',\n",
       "  'linked_terms': ['Artificial Intelligence', 'Precision']},\n",
       " 'wmt-translation-task': {'id': 92,\n",
       "  'term': 'WMT Translation Task',\n",
       "  'definition': 'The principal shared task of the Conference on Machine Translation (WMT), where participants submit MT systems translating between multiple language pairs across various domains, evaluated via human and automatic metrics. [www2.statmt.org], [aclanthology.org]',\n",
       "  'tags': ['Shared Task',\n",
       "   'Machine Translation',\n",
       "   'Evaluation',\n",
       "   'NLP Benchmark'],\n",
       "  'related_terms': ['BLEU', 'Human Evaluation', 'Multilingual MT'],\n",
       "  'examples': [],\n",
       "  'source': 'Official WMT site and ACL Anthology proceedings. [www2.statmt.org], [aclanthology.org]',\n",
       "  'last_updated': '2025-11-02',\n",
       "  'key_slug': 'wmt-translation-task',\n",
       "  'original_term': 'WMT Translation Task',\n",
       "  'linked_terms': ['Artificial Intelligence']},\n",
       " 'ensembles-2': {'id': 93,\n",
       "  'term': 'Ensembles',\n",
       "  'definition': 'Techniques combining predictions from multiple modelsâ€”through bagging, boosting, stacking, or votingâ€”to enhance accuracy, reduce variance, and improve generalization. [geeksforgeeks.org], [arxiv.org]',\n",
       "  'tags': ['Model Aggregation', 'Generalization', 'Robustness', 'NLP Methods'],\n",
       "  'related_terms': ['Bagging', 'Boosting', 'Stacking', 'Random Forests'],\n",
       "  'examples': [],\n",
       "  'source': 'GeeksforGeeks overview (2025); Jia et al. 2023 review on ensemble deep learning in NLP. [geeksforgeeks.org], [arxiv.org]',\n",
       "  'last_updated': '2025-11-02',\n",
       "  'key_slug': 'ensembles-2',\n",
       "  'original_term': 'Ensembles',\n",
       "  'linked_terms': ['Generalization']},\n",
       " 'constituency-parsing': {'id': 94,\n",
       "  'term': 'Constituency Parsing',\n",
       "  'definition': 'The process of analyzing a hierarchical phrase-based syntactic structure of a sentence using context-free grammar (CFG), producing parse trees that represent nested constituents like noun and verb phrases. [numberanalytics.com], [geeksforgeeks.org]',\n",
       "  'tags': ['Syntax', 'Parsing', 'Tree-Structure', 'NLP'],\n",
       "  'related_terms': ['Dependency Parsing', 'CFG', 'PCFG', 'CKY Algorithm'],\n",
       "  'examples': [],\n",
       "  'source': 'Guide on Constituency Parsing (2025); GeeksforGeeks (2025). [numberanalytics.com], [geeksforgeeks.org]',\n",
       "  'last_updated': '2025-11-02',\n",
       "  'key_slug': 'constituency-parsing',\n",
       "  'original_term': 'Constituency Parsing',\n",
       "  'linked_terms': []},\n",
       " 'context-vector': {'id': 95,\n",
       "  'term': 'Context Vector',\n",
       "  'definition': 'A vector based on contextual information from the encoder in sequence-to-sequence models, summarizing input sequences into fixed-length representations for the decoder to generate outputs. Attention mechanisms often enhance context vectors by dynamically weighting encoder hidden states. [stackoverflow.com]',\n",
       "  'tags': ['Encoderâ€“Decoder',\n",
       "   'Fixed-Length Representation',\n",
       "   'Attention',\n",
       "   'Seq2Seq',\n",
       "   'Neural Machine Translation',\n",
       "   'Sequence Modeling',\n",
       "   'Encoders',\n",
       "   'Attention',\n",
       "   'Seq2Seq'],\n",
       "  'related_terms': ['Encoder',\n",
       "   'Decoder',\n",
       "   'Hidden State',\n",
       "   'Attention Mechanism',\n",
       "   'Encoder Hidden States',\n",
       "   'Attention Weights / Alignment Scores',\n",
       "   'Decoder Hidden State',\n",
       "   'Attention Mechanism (Bahdanau',\n",
       "   'Luong)'],\n",
       "  'examples': [],\n",
       "  'source': 'Stack Overflow explanation (2023). [stackoverflow.com]',\n",
       "  'last_updated': '2025-11-02',\n",
       "  'key_slug': 'context-vector',\n",
       "  'original_term': 'Context Vector',\n",
       "  'linked_terms': ['Sequence-to-Sequence Model']},\n",
       " 'recurrence': {'id': 96,\n",
       "  'term': 'Recurrence',\n",
       "  'definition': 'A mechanism in recurrent neural networks (RNNs) where each hidden state depends on both the current input and the previous hidden state, enabling modeling of temporal dependencies in sequence data. [geeksforgeeks.org], [en.wikipedia.org]',\n",
       "  'tags': ['Sequential Modeling', 'RNN', 'Time-Series', 'NLP'],\n",
       "  'related_terms': ['RNN',\n",
       "   'LSTM',\n",
       "   'GRU',\n",
       "   'Backpropagation Through Time',\n",
       "   'Polynomial Time'],\n",
       "  'examples': [],\n",
       "  'source': 'GeeksforGeeks (2025); Wikipedia entry on RNNs. [geeksforgeeks.org], [en.wikipedia.org]',\n",
       "  'last_updated': '2025-11-02',\n",
       "  'key_slug': 'recurrence',\n",
       "  'original_term': 'Recurrence',\n",
       "  'linked_terms': ['Neural Network', 'Recurrent Neural Network']},\n",
       " 'convolutions': {'id': 97,\n",
       "  'term': 'Convolutions',\n",
       "  'definition': \"Local feature extractors applying filters over word sequences to capture nâ€‘gram features; originally common in CNN-based NLP, but in transformers, they're replacedâ€”or augmentedâ€”by self-attention layers that capture global dependencies. [coursera.org], [geeksforgeeks.org]\",\n",
       "  'tags': ['CNN', 'Feature Extraction', 'Local Patterns', 'NLP'],\n",
       "  'related_terms': ['CNN', 'Self-Attention', 'Transformer', 'RNN'],\n",
       "  'examples': [],\n",
       "  'source': 'Coursera (2025); GeeksforGeeks overview (2025). [coursera.org], [geeksforgeeks.org]',\n",
       "  'last_updated': '2025-11-02',\n",
       "  'key_slug': 'convolutions',\n",
       "  'original_term': 'Convolutions',\n",
       "  'linked_terms': ['Neural Network', 'Self-Attention', 'Transformer']},\n",
       " 'transformer': {'id': 98,\n",
       "  'term': 'Transformer',\n",
       "  'definition': 'A neural architecture built on multi-head self-attention and feed-forward layers (no recurrence), enabling parallel sequence processing and capturing long-range dependencies; introduced in â€œAttention Is All You Needâ€ (2017). [en.wikipedia.org], [arxiv.org]',\n",
       "  'tags': ['Self-Attention', 'Encoderâ€“Decoder', 'NLP', 'Parallelism'],\n",
       "  'related_terms': ['Multi-Head Attention',\n",
       "   'Positional Encoding',\n",
       "   'BERT',\n",
       "   'GPT'],\n",
       "  'examples': [],\n",
       "  'source': 'Wikipedia; Tong Xiao & Jingbo Zhu (2023) arXiv survey. [en.wikipedia.org], [arxiv.org]',\n",
       "  'last_updated': '2025-11-02',\n",
       "  'key_slug': 'transformer',\n",
       "  'original_term': 'Transformer',\n",
       "  'linked_terms': ['Recurrence', 'Self-Attention']},\n",
       " 'self-attention': {'id': 99,\n",
       "  'term': 'Self-Attention',\n",
       "  'definition': 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It allows the model to weigh the importance of different words in a sequence when encoding a particular word, capturing long-range dependencies effectively. [en.wikipedia.org], [geeksforgeeks.org]',\n",
       "  'tags': ['Attention', 'Milti-head Attention'],\n",
       "  'related_terms': ['Attention',\n",
       "   'Multi-head Attention',\n",
       "   'neural network',\n",
       "   'transformer'],\n",
       "  'examples': [],\n",
       "  'source': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. (2017), *\"Attention is All You Need\"*, Google, In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS\\'17).',\n",
       "  'last_updated': '2025-11-02',\n",
       "  'key_slug': 'self-attention',\n",
       "  'original_term': 'Self-Attention',\n",
       "  'linked_terms': []}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Step 3: Enrich glossary\n",
    "enrich_glossary(\"data/aiml_glossary.json\", \"data/link_dictionary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e449a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Graph cluster assignments saved: /home/ian/dev/aiml-glossary/data/cluster_assignments.csv\n",
      "ðŸ“ˆ Cluster visualization saved: /home/ian/dev/aiml-glossary/visualizations/glossary_clusters.png\n",
      "ðŸ“Š Graph clustering logged to MLflow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7136da9ba950>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Step 4: Graph clustering\n",
    "run_clustering(\"data/aiml_glossary.json\", \"data/link_dictionary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366c9050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Semantic cluster assignments saved: /home/ian/dev/aiml-glossary/data/semantic_cluster_assignments.csv\n",
      "ðŸ“ˆ Semantic cluster visualization saved: /home/ian/dev/aiml-glossary/visualizations/semantic_clusters.png\n",
      "ðŸ“Š Semantic clustering logged to MLflow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 2, 7, 6, 2, 4, 1, 2, 1, 1, 2, 5, 5, 0, 5, 1, 1, 2, 6, 2,\n",
       "       0, 3, 5, 5, 1, 1, 2, 5, 2, 2, 6, 7, 6, 1, 1, 7, 1, 6, 2, 2, 2, 1,\n",
       "       6, 6, 6, 1, 3, 6, 2, 6, 6, 5, 6, 5, 2, 7, 3, 2, 2, 2, 2, 1, 5, 5,\n",
       "       1, 7, 1, 6, 5, 2, 4, 7, 7, 1, 2, 7, 0, 1, 1, 2, 4, 5, 2, 2, 0, 1,\n",
       "       0, 1, 1, 5, 1, 1, 1, 7, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Step 5: Semantic clustering\n",
    "run_semantic_clustering(\"data/aiml_glossary.json\", n_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa93a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved /home/ian/dev/aiml-glossary/data/graph_stats.json\n",
      "âœ… Saved /home/ian/dev/aiml-glossary/data/ari_metrics.json\n",
      "ðŸ“Š Cluster evaluation logged to MLflow\n",
      "{'graph_stats': {'total_terms': 0, 'agreements': 0, 'agreement_ratio': 0.0}, 'ari_metrics': {'adjusted_rand_index': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# %% Step 6: Evaluate clusters\n",
    "summary = evaluate_clusters()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0f5bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage Report:\n",
      "âœ… data:aiml_glossary.json (True)\n",
      "âœ… data:terms.csv (True)\n",
      "âœ… data:glossary_copy.json (True)\n",
      "âœ… data:link_dictionary.json (True)\n",
      "âœ… data:enriched_glossary.json (True)\n",
      "âœ… data:cluster_assignments.csv (True)\n",
      "âœ… data:semantic_cluster_assignments.csv (True)\n",
      "âœ… data:graph_stats.json (True)\n",
      "âœ… data:ari_metrics.json (True)\n",
      "âœ… data:coverage_report.json (True)\n",
      "âœ… visualizations:glossary_clusters.png (True)\n",
      "âœ… visualizations:semantic_clusters.png (True)\n",
      "\n",
      "Coverage report written to /home/ian/dev/aiml-glossary/data/coverage_report.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data:aiml_glossary.json': True,\n",
       " 'data:terms.csv': True,\n",
       " 'data:glossary_copy.json': True,\n",
       " 'data:link_dictionary.json': True,\n",
       " 'data:enriched_glossary.json': True,\n",
       " 'data:cluster_assignments.csv': True,\n",
       " 'data:semantic_cluster_assignments.csv': True,\n",
       " 'data:graph_stats.json': True,\n",
       " 'data:ari_metrics.json': True,\n",
       " 'data:coverage_report.json': True,\n",
       " 'visualizations:glossary_clusters.png': True,\n",
       " 'visualizations:semantic_clusters.png': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Step 7: Coverage report\n",
    "generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4fe8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage Report:\n",
      "âœ… data:aiml_glossary.json (True)\n",
      "âœ… data:terms.csv (True)\n",
      "âœ… data:glossary_copy.json (True)\n",
      "âœ… data:link_dictionary.json (True)\n",
      "âœ… data:enriched_glossary.json (True)\n",
      "âœ… data:cluster_assignments.csv (True)\n",
      "âœ… data:semantic_cluster_assignments.csv (True)\n",
      "âœ… data:graph_stats.json (True)\n",
      "âœ… data:ari_metrics.json (True)\n",
      "âœ… data:coverage_report.json (True)\n",
      "âœ… visualizations:glossary_clusters.png (True)\n",
      "âœ… visualizations:semantic_clusters.png (True)\n",
      "\n",
      "Coverage report written to /home/ian/dev/aiml-glossary/data/coverage_report.json\n",
      "Workflow âœ… Passed\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "![Workflow Status](https://img.shields.io/badge/Workflow-âœ…%20Passed-brightgreen)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Status Badge\n",
    "from src.coverage_report import generate_report\n",
    "\n",
    "report = generate_report()\n",
    "\n",
    "# Determine overall status\n",
    "if all(v is True for v in report.values()):\n",
    "    badge = \"![Workflow Status](https://img.shields.io/badge/Workflow-âœ…%20Passed-brightgreen)\"\n",
    "    print(\"Workflow âœ… Passed\")\n",
    "else:\n",
    "    badge = \"![Workflow Status](https://img.shields.io/badge/Workflow-âŒ%20Issues-red)\"\n",
    "    print(\"Workflow âŒ Issues detected\")\n",
    "\n",
    "# Display badge inline in notebook\n",
    "from IPython.display import Markdown\n",
    "Markdown(badge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8c4ff",
   "metadata": {},
   "source": [
    "## âœ… Expected Artifact Checklist\n",
    "\n",
    "After running this runbook endâ€‘toâ€‘end, the following artifacts should exist:\n",
    "\n",
    "### ðŸ“‚ Output directory (`output/`)\n",
    "- [ ] `terms.csv` â†’ glossary terms and definitions in CSV format  \n",
    "- [ ] `glossary_copy.json` â†’ copy of the glossary JSON  \n",
    "- [ ] `link_dictionary.json` â†’ generated termâ€‘toâ€‘term link dictionary  \n",
    "- [ ] `enriched_glossary.json` â†’ glossary entries enriched with metadata (length, characters, link counts)  \n",
    "- [ ] `cluster_assignments.csv` â†’ graph clustering assignments  \n",
    "- [ ] `semantic_cluster_assignments.csv` â†’ semantic clustering assignments  \n",
    "- [ ] `graph_stats.json` â†’ node/edge counts and cluster stats from graph clustering  \n",
    "- [ ] `ari_metrics.json` â†’ evaluation metric (Adjusted Rand Index)  \n",
    "- [ ] `coverage_report.json` â†’ summary of which artifacts exist âœ…/âŒ  \n",
    "\n",
    "### ðŸ“‚ Visualizations directory (`visualizations/`)\n",
    "- [ ] `glossary_clusters.png` â†’ graph clustering visualization  \n",
    "- [ ] `semantic_clusters.png` â†’ bar chart of semantic cluster sizes  \n",
    "\n",
    "### ðŸ“Š Diagnostics\n",
    "- Console output should show:\n",
    "  - Graph stats (nodes, edges, clusters)  \n",
    "  - Semantic cluster sizes  \n",
    "  - ARI metric summary  \n",
    "  - Coverage report with âœ…/âŒ markers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glossary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
